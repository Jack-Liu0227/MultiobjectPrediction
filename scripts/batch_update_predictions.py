"""æ‰¹é‡æ›´æ–°é¢„æµ‹ç»“æœè„šæœ¬

å®Œæ•´çš„é¢„æµ‹ç»“æœæ›´æ–°å·¥ä½œæµç¨‹ï¼š
1. ä» process_details.json ä¸­è¯»å–æ‰€æœ‰æ ·æœ¬çš„ llm_response
2. ä½¿ç”¨ LLMResponseParser é‡æ–°è§£æ llm_responseï¼Œæå–é¢„æµ‹å€¼
3. ç”¨é‡æ–°è§£æçš„é¢„æµ‹å€¼æ›´æ–° process_details.json ä¸­çš„ predicted_valuesï¼ˆæ›´æ–°å‰åˆ›å»ºå¤‡ä»½ï¼‰
4. æ›´æ–°æˆ–åˆ›å»º predictions.csvï¼ˆå¦‚ä¸å­˜åœ¨åˆ™ä» test_set.csv åˆ›å»ºï¼‰
5. é‡æ–°è®¡ç®—å¹¶æ›´æ–° metrics.jsonï¼ˆRÂ², RMSE, MAE, MAPEï¼‰

å¤‡ä»½ç­–ç•¥ï¼š
- ä»… process_details.json åœ¨æ›´æ–°å‰åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
- predictions.csv å’Œ metrics.json ç›´æ¥è¦†ç›–ï¼Œä¸åˆ›å»ºå¤‡ä»½

åŠŸèƒ½æ¨¡å—:
1. æ–‡ä»¶è¯»å–æ¨¡å— - è¯»å– process_details.jsonã€predictions.csvã€test_set.csv
2. LLM å“åº”è§£ææ¨¡å— - ä½¿ç”¨ä¼˜åŒ–åçš„è§£æé€»è¾‘
3. ç»“æœå¯¹æ¯”éªŒè¯æ¨¡å— - å¯¹æ¯”è§£æç»“æœä¸å·²ä¿å­˜çš„é¢„æµ‹å€¼
4. CSV/JSON æ–‡ä»¶å†™å…¥æ¨¡å— - ä¿å­˜éªŒè¯æŠ¥å‘Šå’Œæ›´æ–°ç»“æœ
5. æŒ‡æ ‡è®¡ç®—æ¨¡å— - é‡æ–°è®¡ç®—è¯„ä¼°æŒ‡æ ‡
"""
import sys
import json
import shutil
from pathlib import Path
from datetime import datetime
from typing import Dict, List, Tuple, Optional
import pandas as pd
import numpy as np

# æ·»åŠ backendåˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent / "backend"))

from services.simple_rag_engine import SimpleRAGEngine, LLMResponseParser


# ============================================================================
# æ¨¡å—åŒ–ç»„ä»¶
# ============================================================================

class FileReader:
    """æ–‡ä»¶è¯»å–æ¨¡å—"""

    @staticmethod
    def read_process_details(task_dir: Path) -> Optional[List[Dict]]:
        """è¯»å– process_details.json

        Args:
            task_dir: ä»»åŠ¡ç›®å½•

        Returns:
            process_details åˆ—è¡¨ï¼Œå¤±è´¥è¿”å› None
        """
        file_path = task_dir / "process_details.json"
        if not file_path.exists():
            return None

        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"  âš ï¸  è¯»å– process_details.json å¤±è´¥: {e}")
            return None

    @staticmethod
    def read_predictions_csv(task_dir: Path) -> Optional[pd.DataFrame]:
        """è¯»å– predictions.csv

        Args:
            task_dir: ä»»åŠ¡ç›®å½•

        Returns:
            predictions DataFrameï¼Œå¤±è´¥è¿”å› None
        """
        file_path = task_dir / "predictions.csv"
        if not file_path.exists():
            return None

        try:
            return pd.read_csv(file_path)
        except Exception as e:
            print(f"  âš ï¸  è¯»å– predictions.csv å¤±è´¥: {e}")
            return None

    @staticmethod
    def read_test_set_csv(task_dir: Path) -> Optional[pd.DataFrame]:
        """è¯»å– test_set.csv

        Args:
            task_dir: ä»»åŠ¡ç›®å½•

        Returns:
            test_set DataFrameï¼Œå¤±è´¥è¿”å› None
        """
        file_path = task_dir / "test_set.csv"
        if not file_path.exists():
            return None

        try:
            return pd.read_csv(file_path)
        except Exception as e:
            print(f"  âš ï¸  è¯»å– test_set.csv å¤±è´¥: {e}")
            return None


class ResponseParser:
    """LLM å“åº”è§£ææ¨¡å—"""

    def __init__(self):
        self.parser = LLMResponseParser()

    def parse_response(
        self,
        llm_response: str,
        target_columns: List[str]
    ) -> Dict[str, float]:
        """è§£æ LLM å“åº”

        Args:
            llm_response: LLM å“åº”æ–‡æœ¬
            target_columns: ç›®æ ‡å±æ€§åˆ—ååˆ—è¡¨

        Returns:
            è§£æåçš„é¢„æµ‹å€¼å­—å…¸
        """
        return self.parser.parse(llm_response, target_columns)


class ResultComparator:
    """ç»“æœå¯¹æ¯”éªŒè¯æ¨¡å—"""

    @staticmethod
    def compare_predictions(
        parsed_values: Dict[str, float],
        saved_values: Dict[str, float],
        tolerance: float = 1e-6
    ) -> Dict[str, any]:
        """å¯¹æ¯”è§£æç»“æœä¸å·²ä¿å­˜çš„é¢„æµ‹å€¼

        Args:
            parsed_values: è§£æå¾—åˆ°çš„é¢„æµ‹å€¼
            saved_values: å·²ä¿å­˜çš„é¢„æµ‹å€¼
            tolerance: æ•°å€¼æ¯”è¾ƒå®¹å·®

        Returns:
            å¯¹æ¯”ç»“æœå­—å…¸ï¼ŒåŒ…å« is_match, differences ç­‰ä¿¡æ¯
        """
        is_match = True
        differences = {}

        for key in parsed_values.keys():
            parsed_val = parsed_values.get(key, 0.0)
            saved_val = saved_values.get(key, 0.0)

            diff = abs(parsed_val - saved_val)
            differences[key] = {
                'parsed': parsed_val,
                'saved': saved_val,
                'diff': diff
            }

            if diff > tolerance:
                is_match = False

        return {
            'is_match': is_match,
            'differences': differences
        }


class MetricsCalculator:
    """æŒ‡æ ‡è®¡ç®—æ¨¡å—"""

    @staticmethod
    def calculate_metrics(
        df: pd.DataFrame,
        target_columns: List[str]
    ) -> Dict[str, Dict[str, float]]:
        """è®¡ç®—é¢„æµ‹æŒ‡æ ‡

        Args:
            df: åŒ…å«çœŸå®å€¼å’Œé¢„æµ‹å€¼çš„ DataFrame
            target_columns: ç›®æ ‡å±æ€§åˆ—ååˆ—è¡¨

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
        import math

        metrics = {}

        for target_col in target_columns:
            pred_col = f"{target_col}_predicted"

            if pred_col not in df.columns:
                continue

            # ç§»é™¤ç©ºå€¼
            valid_mask = df[target_col].notna() & df[pred_col].notna()
            y_true = df.loc[valid_mask, target_col]
            y_pred = df.loc[valid_mask, pred_col]

            if len(y_true) == 0:
                continue

            # è®¡ç®—æŒ‡æ ‡
            if len(y_true) >= 2:
                r2 = r2_score(y_true, y_pred)
            else:
                r2 = None

            rmse = np.sqrt(mean_squared_error(y_true, y_pred))
            mae = mean_absolute_error(y_true, y_pred)

            # è®¡ç®— MAPE
            with np.errstate(divide='ignore', invalid='ignore'):
                mape = np.mean(np.abs((y_true - y_pred) / y_true)) * 100

            # è½¬æ¢ä¸º JSON å…¼å®¹çš„å€¼
            def safe_float(value):
                if value is None:
                    return None
                if isinstance(value, (int, float)):
                    if math.isnan(value) or math.isinf(value):
                        return None
                    return float(value)
                return value

            metrics[target_col] = {
                "r2": safe_float(r2),
                "rmse": safe_float(rmse),
                "mae": safe_float(mae),
                "mape": safe_float(mape)
            }

        return metrics


class PredictionUpdater:
    """é¢„æµ‹ç»“æœæ›´æ–°å™¨ï¼ˆæ¡†æ¶åŒ–è®¾è®¡ï¼‰"""

    def __init__(self, results_dir: Path):
        self.results_dir = results_dir
        self.target_columns = ["UTS(MPa)", "El(%)"]

        # åˆå§‹åŒ–æ¨¡å—åŒ–ç»„ä»¶
        self.file_reader = FileReader()
        self.response_parser = ResponseParser()
        self.comparator = ResultComparator()
        self.metrics_calculator = MetricsCalculator()

    def find_all_task_dirs(self) -> List[Path]:
        """æŸ¥æ‰¾æ‰€æœ‰ä»»åŠ¡ç›®å½•"""
        task_dirs = []
        if not self.results_dir.exists():
            print(f"âŒ ç»“æœç›®å½•ä¸å­˜åœ¨: {self.results_dir}")
            return task_dirs

        for item in self.results_dir.iterdir():
            if item.is_dir():
                # æ£€æŸ¥æ˜¯å¦åŒ…å«å¿…è¦çš„æ–‡ä»¶ï¼ˆåªéœ€è¦ process_details.jsonï¼‰
                process_details_file = item / "process_details.json"
                if process_details_file.exists():
                    task_dirs.append(item)

        return sorted(task_dirs)

    def verify_task_predictions(self, task_dir: Path) -> pd.DataFrame:
        """éªŒè¯å•ä¸ªä»»åŠ¡çš„é¢„æµ‹ç»“æœ

        Args:
            task_dir: ä»»åŠ¡ç›®å½•

        Returns:
            éªŒè¯è¯¦æƒ… DataFrame
        """
        task_id = task_dir.name
        print(f"\n{'='*80}")
        print(f"éªŒè¯ä»»åŠ¡: {task_id}")
        print(f"{'='*80}")

        # è¯»å– process_details.json
        process_details = self.file_reader.read_process_details(task_dir)
        if not process_details:
            print(f"âŒ æ— æ³•è¯»å– process_details.json")
            return pd.DataFrame()

        print(f"ğŸ“‹ è¯»å– process_details.json: {len(process_details)} æ¡è®°å½•")

        # å‡†å¤‡éªŒè¯ç»“æœåˆ—è¡¨
        verification_results = []

        # éå†æ¯ä¸ªæ ·æœ¬
        for detail in process_details:
            sample_index = detail.get('sample_index')
            llm_response = detail.get('llm_response', '')
            saved_predicted_values = detail.get('predicted_values', {})

            # è§£æ LLM å“åº”
            parsed_values = self.response_parser.parse_response(
                llm_response,
                self.target_columns
            )

            # å¯¹æ¯”ç»“æœ
            comparison = self.comparator.compare_predictions(
                parsed_values,
                saved_predicted_values
            )

            # æ„å»ºéªŒè¯è®°å½•
            result = {
                'sample_index': sample_index,
                'is_match': comparison['is_match'],
            }

            # æ·»åŠ æ¯ä¸ªç›®æ ‡å±æ€§çš„è¯¦ç»†ä¿¡æ¯
            for target_col in self.target_columns:
                diff_info = comparison['differences'].get(target_col, {})
                result[f'{target_col}_parsed'] = diff_info.get('parsed', 0.0)
                result[f'{target_col}_saved'] = diff_info.get('saved', 0.0)
                result[f'{target_col}_diff'] = diff_info.get('diff', 0.0)

            # æ·»åŠ åŒ¹é…åˆ°çš„å®Œæ•´è§£æç»“æœï¼ˆJSON æ ¼å¼å­—ç¬¦ä¸²ï¼‰
            import json
            result['parsed_result'] = json.dumps(parsed_values, ensure_ascii=False)
            result['saved_result'] = json.dumps(saved_predicted_values, ensure_ascii=False)

            verification_results.append(result)

        # è½¬æ¢ä¸º DataFrame
        df_verification = pd.DataFrame(verification_results)

        # ç»Ÿè®¡ä¿¡æ¯
        total_samples = len(df_verification)
        matched_samples = df_verification['is_match'].sum()
        mismatched_samples = total_samples - matched_samples

        print(f"âœ… éªŒè¯å®Œæˆ:")
        print(f"   æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"   åŒ¹é…æ ·æœ¬: {matched_samples}")
        print(f"   ä¸åŒ¹é…æ ·æœ¬: {mismatched_samples}")

        return df_verification

    def update_predictions_and_metrics(
        self,
        task_dir: Path,
        df_verification: pd.DataFrame,
        dry_run: bool = False,
        update_all: bool = False
    ) -> Dict:
        """æ ¹æ®éªŒè¯ç»“æœæ›´æ–° predictions.csv å’Œ metrics.json

        Args:
            task_dir: ä»»åŠ¡ç›®å½•
            df_verification: éªŒè¯è¯¦æƒ… DataFrame
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            update_all: æ˜¯å¦æ›´æ–°æ‰€æœ‰æ ·æœ¬ï¼ˆåŒ…æ‹¬åŒ¹é…çš„æ ·æœ¬ï¼‰

        Returns:
            æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
        """
        task_id = task_dir.name
        stats = {
            'updated_predictions': 0,
            'updated_metrics': False,
            'created_predictions_csv': False
        }

        # è¯»å– process_details.json
        process_details = self.file_reader.read_process_details(task_dir)
        if not process_details:
            print(f"âŒ æ— æ³•è¯»å– process_details.json")
            return stats

        # è¯»å–æˆ–åˆ›å»º predictions.csv
        df_predictions = self.file_reader.read_predictions_csv(task_dir)

        if df_predictions is None:
            # predictions.csv ä¸å­˜åœ¨ï¼Œéœ€è¦ä» test_set.csv åˆ›å»º
            print(f"ğŸ“ predictions.csv ä¸å­˜åœ¨ï¼Œä» test_set.csv åˆ›å»º...")
            df_test_set = self.file_reader.read_test_set_csv(task_dir)

            if df_test_set is None:
                print(f"âŒ æ— æ³•è¯»å– test_set.csvï¼Œæ— æ³•åˆ›å»º predictions.csv")
                return stats

            # åˆ›å»º predictions.csvï¼ŒåŒ…å« test_set.csv çš„æ‰€æœ‰åˆ—
            df_predictions = df_test_set.copy()

            # æ·»åŠ  sample_index åˆ—ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            if 'sample_index' not in df_predictions.columns:
                df_predictions.insert(0, 'sample_index', range(len(df_predictions)))

            # æ·»åŠ é¢„æµ‹å€¼åˆ—
            for target_col in self.target_columns:
                pred_col = f"{target_col}_predicted"
                df_predictions[pred_col] = 0.0

            stats['created_predictions_csv'] = True
            print(f"âœ… å·²åˆ›å»º predictions.csv æ¡†æ¶ï¼ŒåŒ…å« {len(df_predictions)} è¡Œ")
        else:
            print(f"ğŸ“‹ è¯»å–ç°æœ‰ predictions.csv: {len(df_predictions)} è¡Œ")

        # æ ¹æ® update_all å‚æ•°å†³å®šæ›´æ–°å“ªäº›æ ·æœ¬
        if update_all:
            rows_to_update = df_verification
            print(f"ğŸ”„ æ›´æ–°æ‰€æœ‰ {len(rows_to_update)} ä¸ªæ ·æœ¬...")
        else:
            rows_to_update = df_verification[~df_verification['is_match']]
            if len(rows_to_update) == 0:
                print(f"âœ… æ‰€æœ‰æ ·æœ¬é¢„æµ‹å€¼ä¸€è‡´ï¼Œæ— éœ€æ›´æ–°")
                return stats
            print(f"ğŸ”„ æ›´æ–° {len(rows_to_update)} ä¸ªä¸åŒ¹é…æ ·æœ¬...")

        for _, row in rows_to_update.iterrows():
            sample_index = row['sample_index']

            # åœ¨ predictions.csv ä¸­æŸ¥æ‰¾å¯¹åº”è¡Œ
            if 'sample_index' in df_predictions.columns:
                mask = df_predictions['sample_index'] == sample_index
            else:
                mask = df_predictions.index == sample_index

            if not mask.any():
                print(f"  âš ï¸  æœªæ‰¾åˆ°æ ·æœ¬ç´¢å¼• {sample_index}")
                continue

            row_idx = df_predictions[mask].index[0]

            # æ›´æ–°é¢„æµ‹å€¼
            for target_col in self.target_columns:
                pred_col = f"{target_col}_predicted"
                parsed_value = row[f'{target_col}_parsed']

                if pred_col not in df_predictions.columns:
                    df_predictions[pred_col] = 0.0

                old_value = df_predictions.loc[row_idx, pred_col]
                df_predictions.loc[row_idx, pred_col] = parsed_value

                if update_all or old_value != parsed_value:
                    print(f"  âœ“ æ ·æœ¬ {sample_index} - {target_col}: {old_value} â†’ {parsed_value}")

            # æ›´æ–° process_details ä¸­çš„ predicted_values
            for detail in process_details:
                if detail.get('sample_index') == sample_index:
                    detail['predicted_values'] = {
                        target_col: row[f'{target_col}_parsed']
                        for target_col in self.target_columns
                    }
                    break

            stats['updated_predictions'] += 1

        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
        if not dry_run:
            # åˆ›å»ºå¤‡ä»½æ—¶é—´æˆ³
            backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")

            # ä¿å­˜ predictions.csvï¼ˆä¸åˆ›å»ºå¤‡ä»½ï¼‰
            predictions_file = task_dir / "predictions.csv"
            df_predictions.to_csv(predictions_file, index=False, encoding='utf-8')
            if stats['created_predictions_csv']:
                print(f"ğŸ’¾ å·²åˆ›å»º predictions.csv")
            else:
                print(f"ğŸ’¾ å·²æ›´æ–° predictions.csv")

            # å¤‡ä»½å¹¶ä¿å­˜ process_details.jsonï¼ˆä»…æ­¤æ–‡ä»¶éœ€è¦å¤‡ä»½ï¼‰
            process_details_file = task_dir / "process_details.json"
            process_details_backup = task_dir / f"process_details.json.backup"
            shutil.copy2(process_details_file, process_details_backup)

            with open(process_details_file, 'w', encoding='utf-8') as f:
                json.dump(process_details, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ å·²æ›´æ–° process_details.json (å¤‡ä»½: {process_details_backup.name})")

            # é‡æ–°è®¡ç®—å¹¶ä¿å­˜ metrics.jsonï¼ˆä¸åˆ›å»ºå¤‡ä»½ï¼‰
            metrics = self.metrics_calculator.calculate_metrics(
                df_predictions,
                self.target_columns
            )
            metrics_file = task_dir / "metrics.json"
            with open(metrics_file, 'w', encoding='utf-8') as f:
                json.dump(metrics, f, indent=2, ensure_ascii=False)
            print(f"ğŸ’¾ å·²{'æ›´æ–°' if metrics_file.exists() else 'åˆ›å»º'} metrics.json")
            stats['updated_metrics'] = True

        return stats

    def process_task(self, task_dir: Path, dry_run: bool = False, update_all: bool = False) -> Dict:
        """å¤„ç†å•ä¸ªä»»åŠ¡ï¼šéªŒè¯ã€å¯¹æ¯”ã€æ›´æ–°

        Args:
            task_dir: ä»»åŠ¡ç›®å½•
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            update_all: æ˜¯å¦æ›´æ–°æ‰€æœ‰æ ·æœ¬ï¼ˆåŒ…æ‹¬åŒ¹é…çš„æ ·æœ¬ï¼‰

        Returns:
            å¤„ç†ç»Ÿè®¡ä¿¡æ¯
        """
        task_id = task_dir.name

        # æ­¥éª¤1: éªŒè¯é¢„æµ‹ç»“æœ
        df_verification = self.verify_task_predictions(task_dir)

        if df_verification.empty:
            return {
                'task_id': task_id,
                'total_samples': 0,
                'matched_samples': 0,
                'mismatched_samples': 0,
                'updated_samples': 0,
                'errors': 1
            }

        # æ­¥éª¤2: ä¿å­˜éªŒè¯æŠ¥å‘Š
        verification_file = task_dir / f"{task_id}_verification_details.csv"
        if not dry_run:
            df_verification.to_csv(verification_file, index=False, encoding='utf-8')
            print(f"ğŸ“Š å·²ä¿å­˜éªŒè¯æŠ¥å‘Š: {verification_file.name}")

        # æ­¥éª¤3: æ›´æ–°é¢„æµ‹ç»“æœå’ŒæŒ‡æ ‡
        update_stats = self.update_predictions_and_metrics(
            task_dir,
            df_verification,
            dry_run,
            update_all
        )

        # æ±‡æ€»ç»Ÿè®¡ä¿¡æ¯
        total_samples = len(df_verification)
        matched_samples = df_verification['is_match'].sum()
        mismatched_samples = total_samples - matched_samples

        stats = {
            'task_id': task_id,
            'total_samples': total_samples,
            'matched_samples': int(matched_samples),
            'mismatched_samples': int(mismatched_samples),
            'updated_samples': update_stats['updated_predictions'],
            'updated_metrics': update_stats['updated_metrics'],
            'errors': 0
        }

        return stats

    def run_batch_verification(self, dry_run: bool = False, task_filter: str = None, update_all: bool = False):
        """æ‰¹é‡éªŒè¯å’Œæ›´æ–°æ‰€æœ‰ä»»åŠ¡

        Args:
            dry_run: æ˜¯å¦ä¸ºè¯•è¿è¡Œæ¨¡å¼
            task_filter: ä»»åŠ¡IDè¿‡æ»¤å™¨(å¯é€‰,æ”¯æŒéƒ¨åˆ†åŒ¹é…)
            update_all: æ˜¯å¦æ›´æ–°æ‰€æœ‰æ ·æœ¬ï¼ˆåŒ…æ‹¬åŒ¹é…çš„æ ·æœ¬ï¼‰
        """
        print(f"\n{'='*80}")
        print(f"æ‰¹é‡éªŒè¯å’Œæ›´æ–°é¢„æµ‹ç»“æœ")
        print(f"ç»“æœç›®å½•: {self.results_dir}")
        print(f"æ¨¡å¼: {'è¯•è¿è¡Œ(ä¸å†™å…¥æ–‡ä»¶)' if dry_run else 'æ­£å¼è¿è¡Œ'}")
        print(f"æ›´æ–°ç­–ç•¥: {'æ›´æ–°æ‰€æœ‰æ ·æœ¬' if update_all else 'ä»…æ›´æ–°ä¸åŒ¹é…æ ·æœ¬'}")
        if task_filter:
            print(f"è¿‡æ»¤å™¨: {task_filter}")
        print(f"{'='*80}")

        # æŸ¥æ‰¾æ‰€æœ‰ä»»åŠ¡ç›®å½•
        task_dirs = self.find_all_task_dirs()

        if task_filter:
            task_dirs = [d for d in task_dirs if task_filter in d.name]

        if not task_dirs:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½•ä»»åŠ¡ç›®å½•")
            return

        print(f"\næ‰¾åˆ° {len(task_dirs)} ä¸ªä»»åŠ¡ç›®å½•")

        # ç»Ÿè®¡ä¿¡æ¯
        all_stats = []
        total_samples = 0
        total_matched = 0
        total_mismatched = 0
        total_updated = 0
        total_errors = 0

        # å¤„ç†æ¯ä¸ªä»»åŠ¡
        for i, task_dir in enumerate(task_dirs, 1):
            print(f"\n[{i}/{len(task_dirs)}]", end=" ")
            stats = self.process_task(task_dir, dry_run=dry_run, update_all=update_all)
            all_stats.append(stats)

            total_samples += stats['total_samples']
            total_matched += stats['matched_samples']
            total_mismatched += stats['mismatched_samples']
            total_updated += stats['updated_samples']
            total_errors += stats['errors']

        # è¾“å‡ºæ€»ä½“ç»Ÿè®¡
        print(f"\n{'='*80}")
        print(f"æ‰¹é‡éªŒè¯å®Œæˆ")
        print(f"{'='*80}")
        print(f"å¤„ç†ä»»åŠ¡æ•°: {len(task_dirs)}")
        print(f"æ€»æ ·æœ¬æ•°: {total_samples}")
        print(f"åŒ¹é…æ ·æœ¬æ•°: {total_matched}")
        print(f"ä¸åŒ¹é…æ ·æœ¬æ•°: {total_mismatched}")
        print(f"æ›´æ–°æ ·æœ¬æ•°: {total_updated}")
        print(f"é”™è¯¯æ•°: {total_errors}")

        # è¾“å‡ºè¯¦ç»†ç»Ÿè®¡è¡¨
        print(f"\n{'='*80}")
        print(f"è¯¦ç»†ç»Ÿè®¡")
        print(f"{'='*80}")
        print(f"{'ä»»åŠ¡ID':<40} {'æ€»æ•°':>8} {'åŒ¹é…':>8} {'ä¸åŒ¹é…':>8} {'å·²æ›´æ–°':>8} {'é”™è¯¯':>8}")
        print(f"{'-'*80}")

        for stats in all_stats:
            if stats['total_samples'] > 0:
                print(
                    f"{stats['task_id']:<40} "
                    f"{stats['total_samples']:>8} "
                    f"{stats['matched_samples']:>8} "
                    f"{stats['mismatched_samples']:>8} "
                    f"{stats['updated_samples']:>8} "
                    f"{stats['errors']:>8}"
                )

        print(f"{'='*80}")

        # ä¿å­˜ç»Ÿè®¡æŠ¥å‘Š
        if not dry_run:
            report_file = self.results_dir / f"verification_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
            with open(report_file, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': datetime.now().isoformat(),
                    'total_tasks': len(task_dirs),
                    'total_samples': total_samples,
                    'total_matched': total_matched,
                    'total_mismatched': total_mismatched,
                    'total_updated': total_updated,
                    'total_errors': total_errors,
                    'task_stats': all_stats
                }, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ“Š éªŒè¯æŠ¥å‘Šå·²ä¿å­˜: {report_file}")


def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(
        description='æ‰¹é‡éªŒè¯å’Œæ›´æ–°é¢„æµ‹ç»“æœ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
åŠŸèƒ½è¯´æ˜:
  æœ¬è„šæœ¬å®ç°å®Œæ•´çš„é¢„æµ‹ç»“æœæ›´æ–°å·¥ä½œæµç¨‹ï¼š

  1. æ•°æ®æå–å’Œè§£æ
     - ä» process_details.json ä¸­è¯»å–æ‰€æœ‰æ ·æœ¬çš„ llm_response
     - ä½¿ç”¨ LLMResponseParser é‡æ–°è§£æ llm_responseï¼Œæå–é¢„æµ‹å€¼
     - ç”¨é‡æ–°è§£æçš„é¢„æµ‹å€¼æ›´æ–° process_details.json ä¸­çš„ predicted_values

  2. predictions.csv æ–‡ä»¶æ›´æ–°
     - å¦‚æœ predictions.csv å·²å­˜åœ¨ï¼šè¯»å–å¹¶æ›´æ–°é¢„æµ‹å€¼åˆ—
     - å¦‚æœ predictions.csv ä¸å­˜åœ¨ï¼šä» test_set.csv åˆ›å»ºæ–°æ–‡ä»¶

  3. metrics.json æ–‡ä»¶æ›´æ–°
     - ä½¿ç”¨æ›´æ–°åçš„ predictions.csv é‡æ–°è®¡ç®—æ‰€æœ‰è¯„ä¼°æŒ‡æ ‡
     - è®¡ç®— RÂ²ã€RMSEã€MAEã€MAPE ç­‰æŒ‡æ ‡

  4. å¤‡ä»½ç­–ç•¥
     - ä»… process_details.json åœ¨æ›´æ–°å‰åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„å¤‡ä»½
     - å¤‡ä»½æ ¼å¼ï¼šprocess_details.json.backup_YYYYMMDD_HHMMSS
     - predictions.csv å’Œ metrics.json ç›´æ¥è¦†ç›–ï¼Œä¸åˆ›å»ºå¤‡ä»½

  5. éªŒè¯æŠ¥å‘Š
     - ä¿å­˜éªŒè¯è¯¦æƒ…åˆ° {task_id}_verification_details.csv
     - ç”Ÿæˆæ€»ä½“ç»Ÿè®¡æŠ¥å‘Š

ç¤ºä¾‹:
  # è¯•è¿è¡Œæ¨¡å¼(ä¸å®é™…å†™å…¥æ–‡ä»¶ï¼Œä»…æ˜¾ç¤ºå°†è¦æ‰§è¡Œçš„æ“ä½œ)
  python scripts/batch_update_predictions.py --dry-run

  # æ­£å¼è¿è¡Œï¼ŒéªŒè¯å’Œæ›´æ–°æ‰€æœ‰ä»»åŠ¡(ä»…æ›´æ–°ä¸åŒ¹é…çš„æ ·æœ¬)
  python scripts/batch_update_predictions.py

  # æ›´æ–°æ‰€æœ‰æ ·æœ¬(åŒ…æ‹¬è§£æç»“æœä¸å·²ä¿å­˜å€¼ä¸€è‡´çš„æ ·æœ¬)
  python scripts/batch_update_predictions.py --update-all

  # åªå¤„ç†ç‰¹å®šä»»åŠ¡(æ”¯æŒéƒ¨åˆ†åŒ¹é…)
  python scripts/batch_update_predictions.py --filter 4610455b

  # æŒ‡å®šç»“æœç›®å½•
  python scripts/batch_update_predictions.py --results-dir storage/results

  # ç»„åˆä½¿ç”¨
  python scripts/batch_update_predictions.py --filter 4610455b --update-all
        """
    )

    parser.add_argument(
        '--results-dir',
        type=str,
        default='storage/results',
        help='ç»“æœç›®å½•è·¯å¾„ (é»˜è®¤: storage/results)'
    )

    parser.add_argument(
        '--dry-run',
        action='store_true',
        help='è¯•è¿è¡Œæ¨¡å¼,ä¸å®é™…å†™å…¥æ–‡ä»¶'
    )

    parser.add_argument(
        '--filter',
        type=str,
        help='ä»»åŠ¡IDè¿‡æ»¤å™¨(æ”¯æŒéƒ¨åˆ†åŒ¹é…)'
    )

    parser.add_argument(
        '--update-all',
        action='store_true',
        help='æ›´æ–°æ‰€æœ‰æ ·æœ¬(åŒ…æ‹¬åŒ¹é…çš„æ ·æœ¬),é»˜è®¤ä»…æ›´æ–°ä¸åŒ¹é…çš„æ ·æœ¬'
    )

    args = parser.parse_args()

    # è·å–é¡¹ç›®æ ¹ç›®å½•
    script_dir = Path(__file__).parent
    project_root = script_dir.parent
    results_dir = project_root / args.results_dir

    # åˆ›å»ºæ›´æ–°å™¨å¹¶è¿è¡Œ
    updater = PredictionUpdater(results_dir)
    updater.run_batch_verification(
        dry_run=args.dry_run,
        task_filter=args.filter,
        update_all=args.update_all
    )


if __name__ == '__main__':
    main()

