"""
è¿­ä»£é¢„æµ‹æœåŠ¡ - ä½¿ç”¨LangGraphå®ç°è¿­ä»£é¢„æµ‹å·¥ä½œæµ
"""

import logging
import json
import time
from typing import TypedDict, List, Dict, Any, Optional, Set
from datetime import datetime
from pathlib import Path
from concurrent.futures import ThreadPoolExecutor, as_completed
import shutil

import pandas as pd
from langgraph.graph import StateGraph, END
from langgraph.graph.state import CompiledStateGraph

from models.schemas import PredictionConfig, TaskStatus
from services.task_manager import TaskManager
from database.task_db import TaskDatabase
from services.simple_rag_engine import SimpleRAGEngine
from services.prompt_builder import PromptBuilder
from services.convergence_checker import ConvergenceChecker
from services.sample_text_builder import SampleTextBuilder
from config import RESULTS_DIR

logger = logging.getLogger(__name__)


def safe_write_file(file_path: Path, content: str, max_retries: int = 3, retry_delay: float = 0.3) -> bool:
    """
    å®‰å…¨å†™å…¥æ–‡ä»¶ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰

    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        content: æ–‡ä»¶å†…å®¹
        max_retries: æœ€å¤§é‡è¯•æ¬¡æ•°
        retry_delay: é‡è¯•å»¶è¿Ÿï¼ˆç§’ï¼‰

    Returns:
        æ˜¯å¦æˆåŠŸå†™å…¥
    """
    for attempt in range(max_retries):
        try:
            with open(file_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return True
        except PermissionError as e:
            if attempt < max_retries - 1:
                logger.warning(f"æ–‡ä»¶å†™å…¥å¤±è´¥ï¼Œé‡è¯• (å°è¯• {attempt + 1}/{max_retries}): {file_path}")
                time.sleep(retry_delay)
            else:
                logger.error(f"âŒ æ–‡ä»¶æƒé™é”™è¯¯: {e} - {file_path}")
                return False
        except Exception as e:
            logger.error(f"æ–‡ä»¶å†™å…¥å¤±è´¥: {e} - {file_path}")
            return False
    return False


class IterationState(TypedDict):
    """è¿­ä»£é¢„æµ‹çŠ¶æ€å®šä¹‰"""
    
    # ä»»åŠ¡åŸºæœ¬ä¿¡æ¯
    task_id: str
    config: Dict[str, Any]
    
    # æ•°æ®é›†ä¿¡æ¯
    train_data: List[Dict[str, Any]]
    test_data: List[Dict[str, Any]]
    train_embeddings: Any  # numpy array
    
    # è¿­ä»£æ§åˆ¶
    current_iteration: int
    max_iterations: int
    convergence_threshold: float
    early_stop: bool
    
    # é¢„æµ‹ç»“æœ
    iteration_results: Dict[int, Dict[int, Dict[str, float]]]  # {iteration: {sample_idx: {target: value}}}
    iteration_history: Dict[int, Dict[str, List[float]]]  # {sample_idx: {target: [iter1_val, iter2_val, ...]}}
    
    # æ”¶æ•›çŠ¶æ€
    converged_samples: Set[int]
    failed_samples: Dict[int, str]
    
    # LLMé…ç½®
    llm_provider: str
    llm_model: str
    temperature: float
    
    # æ—¶é—´æˆ³
    start_time: datetime
    iteration_start_times: Dict[int, datetime]
    
    # å…¶ä»–é…ç½®
    max_workers: int
    target_properties: List[str]
    sample_size: Optional[int]  # æ¯è½®è¿­ä»£é¢„æµ‹çš„æ ·æœ¬æ•°é‡

    # Promptå’Œå“åº”è®°å½•
    prompts: Dict[int, Dict[int, str]]  # {sample_idx: {iteration: prompt}}
    responses: Dict[int, Dict[int, Dict[str, Any]]]  # {sample_idx: {iteration: response}}


class IterativePredictionService:
    """
    è¿­ä»£é¢„æµ‹æœåŠ¡
    
    ä½¿ç”¨LangGraphå®ç°è¿­ä»£é¢„æµ‹å·¥ä½œæµï¼ŒåŒ…æ‹¬ï¼š
    1. åˆå§‹åŒ–
    2. è¿­ä»£é¢„æµ‹
    3. æ”¶æ•›æ£€æŸ¥
    4. å¤±è´¥å¤„ç†
    5. ç»“æœä¿å­˜
    """
    
    def __init__(
        self,
        task_manager: TaskManager,
        task_db: TaskDatabase,
        rag_engine: SimpleRAGEngine
    ):
        """
        åˆå§‹åŒ–è¿­ä»£é¢„æµ‹æœåŠ¡
        
        Args:
            task_manager: ä»»åŠ¡ç®¡ç†å™¨
            task_db: ä»»åŠ¡æ•°æ®åº“
            rag_engine: RAGå¼•æ“
        """
        self.task_manager = task_manager
        self.task_db = task_db
        self.rag_engine = rag_engine
        self.convergence_checker = ConvergenceChecker()
        
        # æ„å»ºå·¥ä½œæµ
        self.workflow: Optional[CompiledStateGraph] = None
        self._build_graph()

    def run_task(self, task_id: str, file_path: Path, config: PredictionConfig):
        """
        æ‰§è¡Œè¿­ä»£é¢„æµ‹ä»»åŠ¡ï¼ˆåŒ…å«æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼‰
        
        æ”¯æŒå¢é‡é¢„æµ‹ï¼šå¦‚æœ config.continue_from_task_id è®¾ç½®ä¸º task_idï¼Œ
        åˆ™ä¼šåŠ è½½å·²æœ‰çš„é¢„æµ‹ç»“æœå¹¶ç»§ç»­æœªå®Œæˆçš„æ ·æœ¬

        Args:
            task_id: ä»»åŠ¡ID
            file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
            config: é¢„æµ‹é…ç½®
        """
        try:
            logger.info(f"Task {task_id}: Starting iterative prediction task")
            
            # ğŸ”¥ æ£€æŸ¥æ˜¯å¦ä¸ºå¢é‡é¢„æµ‹æ¨¡å¼
            is_incremental = (config.continue_from_task_id == task_id)
            
            if is_incremental:
                logger.info(f"Task {task_id}: Incremental prediction mode detected - will continue from existing results")
            
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.task_manager.update_task(
                task_id,
                {
                    "status": "running",
                    "progress": 0.0,
                    "message": "æ­£åœ¨åŠ è½½æ•°æ®..." if not is_incremental else "æ­£åœ¨åŠ è½½ç°æœ‰é¢„æµ‹ç»“æœ..."
                }
            )

            # 1. åŠ è½½æ•°æ®
            df = pd.read_csv(file_path)
            logger.info(f"Task {task_id}: Loaded {len(df)} samples")

            # 2. è¯†åˆ«ç»„åˆ†åˆ—
            composition_columns = []
            for col in df.columns:
                if any(unit in col.lower() for unit in ['wt%', 'at%']):
                    composition_columns.append(col)

            if not composition_columns:
                # å°è¯•ä½¿ç”¨é…ç½®ä¸­çš„ç»„åˆ†åˆ—
                if config.composition_column:
                    if isinstance(config.composition_column, list):
                        composition_columns = config.composition_column
                    else:
                        composition_columns = [config.composition_column]
                
                if not composition_columns:
                    raise ValueError("æœªæ‰¾åˆ°ç»„åˆ†åˆ—ï¼ˆåº”åŒ…å« wt% æˆ– at%ï¼Œæˆ–åœ¨é…ç½®ä¸­æŒ‡å®šï¼‰")

            logger.info(f"Task {task_id}: Found {len(composition_columns)} composition columns")

            # 3. æ•°æ®é›†åˆ’åˆ†ï¼ˆå¿…é¡»ä½¿ç”¨ç›¸åŒçš„éšæœºç§å­ä»¥ç¡®ä¿ä¸€è‡´æ€§ï¼‰
            from sklearn.model_selection import train_test_split

            train_df, test_df = train_test_split(
                df,
                train_size=config.train_ratio,
                random_state=config.random_seed or 42
            )

            logger.info(
                f"Task {task_id}: Split data into {len(train_df)} train and {len(test_df)} test samples"
            )

            # 4. æ„å»ºæ ·æœ¬æ–‡æœ¬å’ŒåµŒå…¥
            from services.sample_text_builder import SampleTextBuilder

            def format_composition(row, comp_cols):
                """æ ¼å¼åŒ–ç»„åˆ†"""
                comp_parts = []
                for col in comp_cols:
                    value = row[col]
                    element = col.split('(')[0].strip()
                    if value > 0:
                        comp_parts.append(f"{element} {value}")
                return ", ".join(comp_parts)

            # æ„å»ºè®­ç»ƒæ ·æœ¬æ–‡æœ¬
            train_texts = []
            train_data = []

            for idx, row in train_df.iterrows():
                composition_str = format_composition(row, composition_columns)

                # æå–å·¥è‰ºåˆ—
                processing_dict = {}
                if config.processing_column:
                    for proc_col in config.processing_column:
                        if proc_col in row.index and pd.notna(row[proc_col]):
                            processing_dict[proc_col] = row[proc_col]

                # æå–ç‰¹å¾åˆ—
                feature_dict = {}
                if config.feature_columns:
                    for feat_col in config.feature_columns:
                        if feat_col in row.index and pd.notna(row[feat_col]):
                            feature_dict[feat_col] = row[feat_col]

                # æ„å»ºæ ·æœ¬æ–‡æœ¬
                sample_text = SampleTextBuilder.build_sample_text(
                    composition=composition_str,
                    processing_columns=processing_dict if processing_dict else None,
                    feature_columns=feature_dict if feature_dict else None
                )

                train_texts.append(sample_text)

                # ä¿å­˜æ ·æœ¬æ•°æ®
                sample_data = {
                    "composition": composition_str,
                    "sample_text": sample_text
                }

                # æ·»åŠ å·¥è‰ºåˆ—
                if processing_dict:
                    sample_data.update(processing_dict)

                # æ·»åŠ ç‰¹å¾åˆ—
                if feature_dict:
                    sample_data.update(feature_dict)

                # æ·»åŠ ç›®æ ‡å±æ€§
                for target_col in config.target_columns:
                    if target_col in row.index and pd.notna(row[target_col]):
                        sample_data[target_col] = float(row[target_col])

                train_data.append(sample_data)

            # æ„å»ºæµ‹è¯•æ ·æœ¬æ•°æ®ï¼ˆä¿ç•™æ‰€æœ‰åŸå§‹åˆ—ï¼Œç¡®ä¿ CSV æ ¼å¼å®Œæ•´ï¼‰
            test_data = []
            for idx, row in test_df.iterrows():
                composition_str = format_composition(row, composition_columns)

                # æå–å·¥è‰ºåˆ—
                processing_dict = {}
                if config.processing_column:
                    for proc_col in config.processing_column:
                        if proc_col in row.index and pd.notna(row[proc_col]):
                            processing_dict[proc_col] = row[proc_col]

                # æå–ç‰¹å¾åˆ—
                feature_dict = {}
                if config.feature_columns:
                    for feat_col in config.feature_columns:
                        if feat_col in row.index and pd.notna(row[feat_col]):
                            feature_dict[feat_col] = row[feat_col]

                # æ„å»ºæ ·æœ¬æ–‡æœ¬
                sample_text = SampleTextBuilder.build_sample_text(
                    composition=composition_str,
                    processing_columns=processing_dict if processing_dict else None,
                    feature_columns=feature_dict if feature_dict else None
                )

                # ä¿å­˜æ ·æœ¬æ•°æ®ï¼ˆä¿ç•™æ‰€æœ‰åŸå§‹åˆ—ï¼‰
                sample_data = row.to_dict()  # ä¿ç•™æ‰€æœ‰åŸå§‹åˆ—
                sample_data["composition"] = composition_str  # æ·»åŠ æ ¼å¼åŒ–çš„ composition å­—ç¬¦ä¸²
                sample_data["sample_text"] = sample_text  # æ·»åŠ æ ·æœ¬æ–‡æœ¬

                test_data.append(sample_data)

            # 5. ç”ŸæˆåµŒå…¥
            if self.rag_engine:
                self.rag_engine.max_retrieved_samples = config.max_retrieved_samples
                self.rag_engine.similarity_threshold = config.similarity_threshold
            
            train_embeddings = self.rag_engine.create_embeddings(train_texts)

            logger.info(f"Task {task_id}: Generated embeddings for {len(train_texts)} training samples")

            # 6. è¿è¡Œè¿­ä»£é¢„æµ‹ï¼ˆä¼ é€’ continue_from_task_id ä»¥ä¾¿åŠ è½½ç°æœ‰ç»“æœï¼‰
            result = self.run_iterative_prediction(
                task_id=task_id,
                config=config,
                train_data=train_data,
                test_data=test_data,
                train_embeddings=train_embeddings
            )

            if result["success"]:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå®Œæˆ
                self.task_manager.update_task(
                    task_id,
                    {
                        "status": "completed",
                        "progress": 1.0,
                        "message": f"è¿­ä»£é¢„æµ‹å®Œæˆï¼Œå…±{result['total_iterations']}è½®ï¼Œ"
                                   f"æ”¶æ•›{result['converged_samples']}ä¸ªæ ·æœ¬ï¼Œ"
                                   f"å¤±è´¥{result['failed_samples']}ä¸ªæ ·æœ¬"
                    }
                )

                logger.info(f"Task {task_id}: Iterative prediction completed successfully")
            else:
                # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
                self.task_manager.update_task(
                    task_id,
                    {
                        "status": "failed",
                        "error": result.get("error", "æœªçŸ¥é”™è¯¯")
                    }
                )

                logger.error(f"Task {task_id}: Iterative prediction failed: {result.get('error')}")

        except Exception as e:
            logger.error(f"Task {task_id}: Iterative prediction task failed: {e}", exc_info=True)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            self.task_manager.update_task(
                task_id,
                {
                    "status": "failed",
                    "error": str(e)
                }
            )
    
    def _build_graph(self) -> None:
        """æ„å»ºLangGraphå·¥ä½œæµ"""
        
        # åˆ›å»ºçŠ¶æ€å›¾
        workflow = StateGraph(IterationState)
        
        # æ·»åŠ èŠ‚ç‚¹
        workflow.add_node("initialize", self._node_initialize)
        workflow.add_node("predict_iteration", self._node_predict_iteration)
        workflow.add_node("check_convergence", self._node_check_convergence)
        workflow.add_node("save_results", self._node_save_results)
        workflow.add_node("handle_failure", self._node_handle_failure)
        
        # è®¾ç½®å…¥å£ç‚¹
        workflow.set_entry_point("initialize")
        
        # æ·»åŠ è¾¹
        workflow.add_edge("initialize", "predict_iteration")
        
        # æ¡ä»¶è·¯ç”±ï¼šé¢„æµ‹åæ£€æŸ¥æ˜¯å¦æœ‰å¤±è´¥
        workflow.add_conditional_edges(
            "predict_iteration",
            self._should_handle_failure,
            {
                "handle_failure": "handle_failure",
                "continue": "check_convergence"
            }
        )
        
        # å¤±è´¥å¤„ç†åç»§ç»­æ”¶æ•›æ£€æŸ¥
        workflow.add_edge("handle_failure", "check_convergence")
        
        # æ¡ä»¶è·¯ç”±ï¼šæ£€æŸ¥æ˜¯å¦ç»§ç»­è¿­ä»£
        workflow.add_conditional_edges(
            "check_convergence",
            self._should_continue_iteration,
            {
                "continue": "predict_iteration",
                "finish": "save_results"
            }
        )
        
        # ä¿å­˜ç»“æœåç»“æŸ
        workflow.add_edge("save_results", END)
        
        # ç¼–è¯‘å·¥ä½œæµ
        self.workflow = workflow.compile()
        logger.info("LangGraphå·¥ä½œæµæ„å»ºå®Œæˆ")

    def _node_initialize(self, state: IterationState) -> IterationState:
        """
        åˆå§‹åŒ–èŠ‚ç‚¹ - åˆå§‹åŒ–è¿­ä»£é¢„æµ‹çš„çŠ¶æ€
        """
        task_id = state['task_id']
        logger.info(f"Task {task_id}: åˆå§‹åŒ–è¿­ä»£é¢„æµ‹")

        # åˆå§‹åŒ–è¿­ä»£ç»“æœå­˜å‚¨
        state["iteration_results"] = {}
        state["iteration_history"] = {}
        state["converged_samples"] = set()
        state["failed_samples"] = {}
        state["iteration_start_times"] = {}
        state["current_iteration"] = 1
        state["start_time"] = datetime.now()

        # æ›´æ–°ä»»åŠ¡çŠ¶æ€ï¼ˆè®¾ç½®åˆå§‹è¿›åº¦ä¸º 0.0ï¼Œå¹¶è®¾ç½® result_idï¼‰
        self.task_manager.update_task(
            task_id,
            {
                "status": TaskStatus.RUNNING,
                "progress": 0.0,
                "message": f"å¼€å§‹è¿­ä»£é¢„æµ‹ï¼ˆæœ€å¤§{state['max_iterations']}è½®ï¼‰",
                "result_id": task_id  # è®¾ç½® result_idï¼Œä½¿å‰ç«¯å¯ä»¥ç«‹å³è®¿é—®ç»“æœ
            }
        )

        # åŒæ—¶æ›´æ–°æ•°æ®åº“ä¸­çš„ result_id
        self.task_db.update_task(task_id, {"result_id": task_id})
        logger.info(f"Task {task_id}: å·²è®¾ç½® result_id")

        logger.info(
            f"Task {task_id}: åˆå§‹åŒ–å®Œæˆï¼Œ"
            f"æµ‹è¯•æ ·æœ¬æ•°={len(state['test_data'])}, "
            f"æœ€å¤§è¿­ä»£æ¬¡æ•°={state['max_iterations']}"
        )

        # æ£€æŸ¥æ˜¯å¦æ˜¯å¢é‡é¢„æµ‹
        config = state['config']
        # ğŸ”¥ ä¿®å¤ï¼šconfig æ˜¯å­—å…¸ï¼ˆæ¥è‡ª model_dump()ï¼‰ï¼Œä½¿ç”¨å­—å…¸æ–¹å¼è®¿é—®
        continue_from_task_id = config.get("continue_from_task_id")
        
        if continue_from_task_id:
            logger.info(f"Task {task_id}: å¢é‡é¢„æµ‹æ¨¡å¼ï¼Œå°è¯•ä»ä»»åŠ¡ {continue_from_task_id} æ¢å¤çŠ¶æ€")
            src_dir = RESULTS_DIR / continue_from_task_id
            dst_dir = RESULTS_DIR / task_id
            
            # å¦‚æœæ˜¯è·¨ä»»åŠ¡ï¼ˆæ–°ä»»åŠ¡ID != æ—§ä»»åŠ¡IDï¼‰ï¼Œåˆ™éœ€è¦å¤åˆ¶æ–‡ä»¶
            if continue_from_task_id != task_id:
                if src_dir.exists():
                    try:
                        # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
                        dst_dir.mkdir(parents=True, exist_ok=True)
                        
                        # 1. å¤åˆ¶ iteration_history.json
                        history_file = src_dir / "iteration_history.json"
                        if history_file.exists():
                            shutil.copy2(history_file, dst_dir / "iteration_history.json")
                            
                        # 2. å¤åˆ¶ inputs å’Œ outputs ç›®å½•ï¼ˆä¿ç•™ Prompt å’Œ Response å†å²ï¼‰
                        if (src_dir / "inputs").exists():
                            if (dst_dir / "inputs").exists():
                                shutil.rmtree(dst_dir / "inputs")
                            shutil.copytree(src_dir / "inputs", dst_dir / "inputs")
                            
                        if (src_dir / "outputs").exists():
                            if (dst_dir / "outputs").exists():
                                shutil.rmtree(dst_dir / "outputs")
                            shutil.copytree(src_dir / "outputs", dst_dir / "outputs")
                    except Exception as e:
                        logger.error(f"Task {task_id}: å¤åˆ¶å†å²æ–‡ä»¶å¤±è´¥: {e}", exc_info=True)
                else:
                    logger.warning(f"Task {task_id}: æŒ‡å®šçš„æ—§ä»»åŠ¡ {continue_from_task_id} ç›®å½•ä¸å­˜åœ¨")
            
            # 3. åŠ è½½å†å²æ•°æ®å¹¶æ¢å¤çŠ¶æ€ (æ— è®ºæ˜¯åŸåœ°è¿˜æ˜¯è·¨ä»»åŠ¡ï¼Œåªè¦æ–‡ä»¶åœ¨ dst_dir å°±åŠ è½½)
            try:
                if (dst_dir / "iteration_history.json").exists():
                    with open(dst_dir / "iteration_history.json", 'r', encoding='utf-8') as f:
                        history_json = json.load(f)
                    
                    self._restore_state_from_history(state, history_json)
                    
                    # å…³é”®ï¼šé‡ç½®å¤±è´¥æ ·æœ¬ï¼Œä»¥ä¾¿åœ¨æœ¬æ¬¡å¢é‡é¢„æµ‹ä¸­é‡è¯•
                    state["failed_samples"] = {}
                    
                    # å…³é”®ï¼šé‡ç½®å½“å‰è½®æ¬¡ä¸º1ï¼Œä»¥ä¾¿ä»å¤´æ‰«æå¹¶è¡¥å…¨ç¼ºå¤±çš„é¢„æµ‹
                    state["current_iteration"] = 1
                    
                    logger.info(f"Task {task_id}: å·²æ¢å¤å†å²çŠ¶æ€ï¼Œå‡†å¤‡è¿›è¡Œå¢é‡é¢„æµ‹ï¼ˆå¤±è´¥æ ·æœ¬å·²é‡ç½®ï¼‰")
            except Exception as e:
                logger.error(f"Task {task_id}: æ¢å¤å†å²æ•°æ®å¤±è´¥: {e}", exc_info=True)

        return state

    def _restore_state_from_history(self, state: IterationState, history_json: Dict[str, Any]):
        """ä»å†å²JSONæ¢å¤çŠ¶æ€"""
        samples_data = history_json.get("samples", {})
        
        # æ¢å¤ iteration_history
        for sample_key, sample_info in samples_data.items():
            # sample_key æ ¼å¼ä¸º "sample_0", "sample_1" ç­‰
            try:
                sample_idx = int(sample_key.split("_")[1])
            except:
                continue
                
            targets_info = sample_info.get("targets", {})
            
            # é‡å»ºè¯¥æ ·æœ¬çš„å†å²è®°å½•
            sample_history = {}
            is_converged = False
            
            for target, info in targets_info.items():
                iterations = info.get("iterations", [])
                sample_history[target] = iterations
                
                if info.get("convergence_status") == "converged":
                    is_converged = True
            
            if sample_history:
                state["iteration_history"][sample_idx] = sample_history
            
            if is_converged:
                state["converged_samples"].add(sample_idx)
        
        # å°è¯•æ¢å¤ prompts å’Œ responses (ä»æ–‡ä»¶ç³»ç»Ÿè¯»å–å¯èƒ½å¤ªæ…¢ï¼Œè¿™é‡Œåªæ¢å¤å†…å­˜ä¸­çš„ç»“æ„ä»¥ä¾¿åç»­è¿½åŠ )
        # æ³¨æ„ï¼šå¦‚æœä¸æ¢å¤ prompts/responses åˆ°å†…å­˜ï¼Œ_build_sample_detail æ—¶å¯èƒ½ä¼šç¼ºå¤±æ—§è½®æ¬¡çš„ä¿¡æ¯
        # ä½†ç”±äºæˆ‘ä»¬å·²ç»å¤åˆ¶äº† inputs/outputs æ–‡ä»¶å¤¹ï¼Œä¸” _save_prompts_and_responses æ˜¯è¿½åŠ å†™å…¥ï¼ˆæˆ–è¦†ç›–ï¼‰ï¼Œ
        # åªè¦æˆ‘ä»¬ä¸è¦†ç›–æ—§æ–‡ä»¶ï¼Œæˆ–è€…é‡æ–°è¯»å–å®ƒä»¬ã€‚
        # 
        # å®é™…ä¸Šï¼Œ_build_sample_detail ä¾èµ– state["prompts"] å’Œ state["responses"]ã€‚
        # ä¸ºäº†ç”Ÿæˆå®Œæ•´çš„ process_details.jsonï¼Œæˆ‘ä»¬éœ€è¦æŠŠæ—§çš„ prompt/response åŠ è½½åˆ°å†…å­˜ã€‚
        # è¿™é‡Œåšä¸€ä¸ªç®€å•çš„åŠ è½½ï¼šè¯»å– inputs/outputs ç›®å½•ä¸‹çš„æ–‡ä»¶
        
        task_id = state["task_id"]
        result_dir = RESULTS_DIR / task_id
        
        inputs_dir = result_dir / "inputs"
        outputs_dir = result_dir / "outputs"
        
        if inputs_dir.exists():
            for sample_dir in inputs_dir.iterdir():
                if sample_dir.is_dir() and sample_dir.name.startswith("sample_"):
                    try:
                        idx = int(sample_dir.name.split("_")[1])
                        if idx not in state["prompts"]:
                            state["prompts"][idx] = {}
                            
                        for file in sample_dir.glob("iteration_*.txt"):
                            iter_num = int(file.stem.split("_")[1])
                            with open(file, 'r', encoding='utf-8') as f:
                                state["prompts"][idx][iter_num] = f.read()
                    except:
                        pass

        if outputs_dir.exists():
            for sample_dir in outputs_dir.iterdir():
                if sample_dir.is_dir() and sample_dir.name.startswith("sample_"):
                    try:
                        idx = int(sample_dir.name.split("_")[1])
                        if idx not in state["responses"]:
                            state["responses"][idx] = {}
                            
                        for file in sample_dir.glob("iteration_*.txt"):
                            iter_num = int(file.stem.split("_")[1])
                            with open(file, 'r', encoding='utf-8') as f:
                                content = f.read()
                                # æ„é€ ä¸€ä¸ªç®€å•çš„ response å¯¹è±¡
                                state["responses"][idx][iter_num] = {
                                    "llm_response": content,
                                    "predictions": {}, # æ— æ³•ä»æ–‡æœ¬æ¢å¤é¢„æµ‹å€¼ï¼Œä½†è¿™é€šå¸¸ä¸å½±å“æ˜¾ç¤º
                                    "confidence": None
                                }
                    except:
                        pass

    def _get_candidate_samples(self, state: IterationState) -> List[tuple]:
        """
        è·å–å€™é€‰æ ·æœ¬ï¼ˆæ’é™¤å®Œå…¨å®Œæˆçš„æ ·æœ¬ï¼‰
        
        å¢é‡é¢„æµ‹åœºæ™¯ï¼š
        - å·²æ”¶æ•›ä½†è¿­ä»£å†å²ä¸å®Œæ•´çš„æ ·æœ¬åº”è¯¥è¢«åŒ…å«ï¼ˆä»¥ä¾¿è¡¥å…¨ç¼ºå¤±çš„è¿­ä»£ï¼‰
        - early_stop=false æ—¶ï¼Œå³ä½¿å·²æ”¶æ•›ä¹Ÿè¦ç»§ç»­åˆ° max_iterations
        - åªæœ‰è¿­ä»£å†å²å®Œæ•´ï¼ˆiterationsæ•°ç»„é•¿åº¦ == max_iterationsï¼‰ä¸”å·²æ”¶æ•›çš„æ ·æœ¬æ‰è¢«æ’é™¤

        Returns:
            å€™é€‰æ ·æœ¬åˆ—è¡¨ [(idx, test_sample), ...]
        """
        candidate_samples = []
        sample_size = state.get("sample_size")
        max_iterations = state.get("max_iterations", 1)
        early_stop = state.get("early_stop", True)
        
        task_id = state.get("task_id", "unknown")
        logger.info(f"Task {task_id}: ç­›é€‰å€™é€‰æ ·æœ¬ (sample_size={sample_size}, max_iterations={max_iterations}, early_stop={early_stop})")
        
        for idx, test_sample in enumerate(state["test_data"]):
            # å¦‚æœ sample_size å­˜åœ¨ï¼Œåªå¤„ç†å‰ sample_size ä¸ªæ ·æœ¬
            # è¿™æ˜¯ä¸€ä¸ªç¡¬æ€§é™åˆ¶ï¼šåªæœ‰å‰ sample_size ä¸ªæ ·æœ¬ä¼šè¢«çº³å…¥é¢„æµ‹èŒƒå›´
            if sample_size is not None and sample_size > 0 and idx >= sample_size:
                logger.debug(f"Task {task_id}: æ ·æœ¬{idx} - è¶…å‡º sample_size èŒƒå›´ï¼Œè·³è¿‡")
                continue
            
            # ğŸ”¥ ä¿®æ”¹ï¼šæ£€æŸ¥æ ·æœ¬æ˜¯å¦å®Œå…¨å®Œæˆ
            # å®Œå…¨å®Œæˆ = å·²æ”¶æ•› + è¿­ä»£å†å²å®Œæ•´ (ä»…åœ¨ early_stop=true æ—¶)
            should_exclude = False
            exclude_reason = ""
            
            if idx in state["failed_samples"]:
                # å¤±è´¥æ ·æœ¬åº”è¯¥è¢«åŒ…å«ï¼Œä»¥ä¾¿é‡è¯•
                should_exclude = False
                logger.info(f"Task {task_id}: æ ·æœ¬{idx} - å¤±è´¥æ ·æœ¬ï¼ŒåŒ…å«ä»¥ä¾¿é‡è¯•")
            elif idx in state["converged_samples"]:
                # ğŸ”¥ å…³é”®ï¼šå¦‚æœ early_stop=falseï¼Œå·²æ”¶æ•›çš„æ ·æœ¬ä¹Ÿè¦ç»§ç»­
                if not early_stop:
                    # early_stop=falseï¼šæ£€æŸ¥è¿­ä»£å†å²æ˜¯å¦å®Œæ•´
                    if idx in state["iteration_history"]:
                        history = state["iteration_history"][idx]
                        # æ£€æŸ¥æ‰€æœ‰ç›®æ ‡å±æ€§çš„è¿­ä»£å†å²é•¿åº¦
                        all_complete = True
                        for target in state["target_properties"]:
                            vals = history.get(target, [])
                            current_len = len(vals)
                            # å¦‚æœä»»ä½•ç›®æ ‡å±æ€§çš„è¿­ä»£æ•° < max_iterationsï¼Œåˆ™ä¸å®Œæ•´
                            if current_len < max_iterations:
                                all_complete = False
                                logger.info(f"Task {task_id}: æ ·æœ¬{idx} - å·²æ”¶æ•›ä½†å†å²ä¸å®Œæ•´ ({target}: {current_len}/{max_iterations})ï¼ŒåŒ…å«")
                                break
                            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸€è½®çš„å€¼æ˜¯ None
                            if any(v is None for v in vals):
                                all_complete = False
                                logger.info(f"Task {task_id}: æ ·æœ¬{idx} - å·²æ”¶æ•›ä½†æœ‰ None å€¼ ({target})ï¼ŒåŒ…å«")
                                break
                        
                        if all_complete:
                            should_exclude = True
                            exclude_reason = f"å·²æ”¶æ•›ä¸”å†å²å®Œæ•´ ({len(vals)}/{max_iterations})"
                    else:
                        # æ²¡æœ‰å†å²è®°å½•ï¼Œä¸åº”è¯¥æ’é™¤
                        should_exclude = False
                        logger.info(f"Task {task_id}: æ ·æœ¬{idx} - å·²æ”¶æ•›ä½†æ— å†å²è®°å½•ï¼ŒåŒ…å«")
                else:
                    # early_stop=trueï¼šå·²æ”¶æ•›çš„æ ·æœ¬å¯ä»¥æ’é™¤ï¼ˆä½†è¿˜è¦æ£€æŸ¥å†å²å®Œæ•´æ€§ï¼‰
                    if idx in state["iteration_history"]:
                        history = state["iteration_history"][idx]
                        # æ£€æŸ¥æ‰€æœ‰ç›®æ ‡å±æ€§çš„è¿­ä»£å†å²é•¿åº¦
                        all_complete = True
                        for target in state["target_properties"]:
                            vals = history.get(target, [])
                            current_len = len(vals)
                            # å¦‚æœä»»ä½•ç›®æ ‡å±æ€§çš„è¿­ä»£æ•° < max_iterationsï¼Œåˆ™ä¸å®Œæ•´
                            if current_len < max_iterations:
                                all_complete = False
                                logger.info(f"Task {task_id}: æ ·æœ¬{idx} - å·²æ”¶æ•›ä½†å†å²ä¸å®Œæ•´ ({target}: {current_len}/{max_iterations})ï¼ŒåŒ…å«")
                                break
                            # æ£€æŸ¥æ˜¯å¦æœ‰ä»»ä½•ä¸€è½®çš„å€¼æ˜¯ None
                            if any(v is None for v in vals):
                                all_complete = False
                                logger.info(f"Task {task_id}: æ ·æœ¬{idx} - å·²æ”¶æ•›ä½†æœ‰ None å€¼ ({target})ï¼ŒåŒ…å«")
                                break
                        
                        if all_complete:
                            should_exclude = True
                            exclude_reason = f"å·²æ”¶æ•›ä¸”å†å²å®Œæ•´ ({len(vals)}/{max_iterations})"
                    else:
                        # æ²¡æœ‰å†å²è®°å½•ï¼Œä¸åº”è¯¥æ’é™¤
                        should_exclude = False
                        logger.info(f"Task {task_id}: æ ·æœ¬{idx} - å·²æ”¶æ•›ä½†æ— å†å²è®°å½•ï¼ŒåŒ…å«")
            else:
                # æ—¢æœªæ”¶æ•›ä¹Ÿæœªå¤±è´¥çš„æ ·æœ¬
                logger.info(f"Task {task_id}: æ ·æœ¬{idx} - æœªæ”¶æ•›ï¼ŒåŒ…å«")
            
            if should_exclude:
                logger.info(f"Task {task_id}: æ ·æœ¬{idx} - æ’é™¤ï¼ˆ{exclude_reason}ï¼‰")
            else:
                candidate_samples.append((idx, test_sample))
        
        logger.info(f"Task {task_id}: ç­›é€‰ç»“æœ - {len(candidate_samples)} ä¸ªå€™é€‰æ ·æœ¬: {[idx for idx, _ in candidate_samples]}")
        return candidate_samples

    def _select_samples_to_predict(
        self,
        state: IterationState,
        candidate_samples: List[tuple],
        current_iter: int
    ) -> List[tuple]:
        """
        æ ¹æ® sample_size å‚æ•°é€‰æ‹©æœ¬è½®è¦é¢„æµ‹çš„æ ·æœ¬ï¼ˆé¡ºåºé€‰æ‹©ï¼Œä¸éšæœºï¼‰
        
        å¢é‡é¢„æµ‹é€»è¾‘ï¼š
        - å¦‚æœæŸä¸ªæ ·æœ¬çš„å½“å‰è½®æ¬¡å·²æœ‰æœ‰æ•ˆé¢„æµ‹å€¼ï¼Œåˆ™è·³è¿‡
        - å¦‚æœå½“å‰è½®æ¬¡çš„å€¼æ˜¯ None æˆ–ä¸å­˜åœ¨ï¼Œåˆ™éœ€è¦é‡æ–°é¢„æµ‹

        Args:
            state: è¿­ä»£çŠ¶æ€
            candidate_samples: å€™é€‰æ ·æœ¬åˆ—è¡¨ [(idx, test_sample), ...]
            current_iter: å½“å‰è¿­ä»£è½®æ¬¡

        Returns:
            æœ¬è½®è¦é¢„æµ‹çš„æ ·æœ¬åˆ—è¡¨
        """
        if state["sample_size"] is not None and state["sample_size"] > 0:
            # é¡ºåºé€‰æ‹©å‰ sample_size ä¸ªæ ·æœ¬ï¼ˆæŒ‰ç´¢å¼•ä»å°åˆ°å¤§ï¼‰
            sorted_candidates = sorted(candidate_samples, key=lambda x: x[0])
            
            # è¿‡æ»¤æ‰åœ¨å½“å‰è½®æ¬¡å·²æœ‰æœ‰æ•ˆç»“æœçš„æ ·æœ¬ï¼ˆå¢é‡é¢„æµ‹é€»è¾‘ï¼‰
            real_candidates = []
            skipped_count = 0
            
            for idx, sample in sorted_candidates:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å½“å‰è½®æ¬¡çš„æœ‰æ•ˆç»“æœ
                has_valid_result = False
                if idx in state["iteration_history"]:
                    history = state["iteration_history"][idx]
                    # æ£€æŸ¥æ‰€æœ‰ç›®æ ‡å±æ€§æ˜¯å¦éƒ½æœ‰å½“å‰è½®æ¬¡çš„æœ‰æ•ˆå€¼ï¼ˆä¸æ˜¯ Noneï¼‰
                    all_targets_have_valid_value = True
                    for target in state["target_properties"]:
                        vals = history.get(target, [])
                        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¸ä»…æ£€æŸ¥é•¿åº¦ï¼Œè¿˜è¦æ£€æŸ¥å€¼çš„æœ‰æ•ˆæ€§
                        if len(vals) < current_iter:
                            # ç¼ºå°‘å½“å‰è½®æ¬¡çš„å€¼
                            all_targets_have_valid_value = False
                            break
                        # æ£€æŸ¥å½“å‰è½®æ¬¡çš„å€¼æ˜¯å¦ä¸º None
                        current_iter_value = vals[current_iter - 1]  # current_iter æ˜¯ 1-indexed
                        if current_iter_value is None:
                            # å½“å‰è½®æ¬¡çš„å€¼æ˜¯ Noneï¼ˆå¤±è´¥ï¼‰ï¼Œéœ€è¦é‡æ–°é¢„æµ‹
                            all_targets_have_valid_value = False
                            break
                    if all_targets_have_valid_value:
                        has_valid_result = True
                
                if not has_valid_result:
                    real_candidates.append((idx, sample))
                else:
                    skipped_count += 1
            
            # ä»å‰©ä¸‹çš„å€™é€‰è€…ä¸­é€‰æ‹©
            num_to_predict = min(state["sample_size"], len(real_candidates))
            samples_to_predict = real_candidates[:num_to_predict]

            selected_indices = [idx for idx, _ in samples_to_predict]
            logger.info(
                f"Task {state['task_id']}: ç¬¬{current_iter}è½® - "
                f"å€™é€‰{len(candidate_samples)}ä¸ªï¼Œè·³è¿‡å·²å®Œæˆ{skipped_count}ä¸ªï¼Œ"
                f"è®¡åˆ’é¢„æµ‹{num_to_predict}ä¸ª (sample_size={state['sample_size']}), "
                f"é€‰ä¸­ç´¢å¼•: {selected_indices}"
            )
        else:
            # å¤„ç†æ‰€æœ‰æ ·æœ¬
            real_candidates = []
            skipped_count = 0
            
            for idx, sample in candidate_samples:
                # æ£€æŸ¥æ˜¯å¦å·²æœ‰å½“å‰è½®æ¬¡çš„æœ‰æ•ˆç»“æœ
                has_valid_result = False
                if idx in state["iteration_history"]:
                    history = state["iteration_history"][idx]
                    all_targets_have_valid_value = True
                    for target in state["target_properties"]:
                        vals = history.get(target, [])
                        # ğŸ”¥ å…³é”®ä¿®æ”¹ï¼šä¸ä»…æ£€æŸ¥é•¿åº¦ï¼Œè¿˜è¦æ£€æŸ¥å€¼çš„æœ‰æ•ˆæ€§
                        if len(vals) < current_iter:
                            all_targets_have_valid_value = False
                            break
                        # æ£€æŸ¥å½“å‰è½®æ¬¡çš„å€¼æ˜¯å¦ä¸º None
                        current_iter_value = vals[current_iter - 1]
                        if current_iter_value is None:
                            all_targets_have_valid_value = False
                            break
                    if all_targets_have_valid_value:
                        has_valid_result = True
                
                if not has_valid_result:
                    real_candidates.append((idx, sample))
                else:
                    skipped_count += 1
            
            samples_to_predict = real_candidates
            logger.info(
                f"Task {state['task_id']}: ç¬¬{current_iter}è½® - "
                f"è·³è¿‡å·²å®Œæˆ{skipped_count}ä¸ªï¼Œé¢„æµ‹å‰©ä½™{len(samples_to_predict)}ä¸ªæœªæ”¶æ•›æ ·æœ¬"
            )

        return samples_to_predict

    def _node_predict_iteration(self, state: IterationState) -> IterationState:
        """
        é¢„æµ‹è¿­ä»£èŠ‚ç‚¹ - æ ¹æ® sample_size å‚æ•°é€‰æ‹©æ ·æœ¬è¿›è¡Œé¢„æµ‹
        """
        task_id = state['task_id']
        current_iter = state["current_iteration"]
        logger.info(f"Task {task_id}: å¼€å§‹ç¬¬{current_iter}è½®è¿­ä»£é¢„æµ‹")

        state["iteration_start_times"][current_iter] = datetime.now()

        # è·å–å€™é€‰æ ·æœ¬å¹¶é€‰æ‹©æœ¬è½®è¦é¢„æµ‹çš„æ ·æœ¬
        candidate_samples = self._get_candidate_samples(state)
        samples_to_predict = self._select_samples_to_predict(state, candidate_samples, current_iter)

        # å¹¶è¡Œé¢„æµ‹
        iteration_predictions = self._run_parallel_predictions(state, samples_to_predict, current_iter)

        # ä¿å­˜æœ¬è½®è¿­ä»£ç»“æœ
        state["iteration_results"][current_iter] = iteration_predictions
        self._save_iteration_results(state, current_iter)

        # æ›´æ–°ä»»åŠ¡è¿›åº¦
        self._update_iteration_progress(state, current_iter, len(iteration_predictions))

        return state

    def _run_parallel_predictions(
        self,
        state: IterationState,
        samples_to_predict: List[tuple],
        current_iter: int
    ) -> Dict[int, Dict[str, float]]:
        """
        å¹¶è¡Œæ‰§è¡Œæ ·æœ¬é¢„æµ‹

        Returns:
            é¢„æµ‹ç»“æœå­—å…¸ {sample_idx: {target: value}}
        """
        task_id = state['task_id']
        iteration_predictions = {}
        total_samples = len(state["test_data"])
        completed_count = 0

        with ThreadPoolExecutor(max_workers=state["max_workers"]) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            futures = {
                executor.submit(
                    self._predict_single_sample,
                    state,
                    sample_idx,
                    test_sample,
                    current_iter
                ): sample_idx
                for sample_idx, test_sample in samples_to_predict
            }

            # æ”¶é›†ç»“æœ
            for future in as_completed(futures):
                sample_idx = futures[future]
                try:
                    # è·å–å®Œæ•´ç»“æœï¼ˆåŒ…å«é¢„æµ‹å€¼ã€Promptã€å“åº”ç­‰ï¼‰
                    result_data = future.result()
                    predictions = result_data["predictions"]
                    
                    # æ›´æ–°è¿­ä»£ç»“æœ
                    iteration_predictions[sample_idx] = predictions

                    # æ›´æ–°çŠ¶æ€ä¸­çš„ Prompts å’Œ Responses
                    if sample_idx not in state["prompts"]:
                        state["prompts"][sample_idx] = {}
                    state["prompts"][sample_idx][current_iter] = result_data["prompt"]

                    if sample_idx not in state["responses"]:
                        state["responses"][sample_idx] = {}
                    state["responses"][sample_idx][current_iter] = result_data["response_data"]

                    # æ£€æŸ¥é¢„æµ‹å€¼æ˜¯å¦æœ‰æ•ˆï¼ˆéé›¶ï¼‰
                    all_zeros = True
                    for target in state["target_properties"]:
                        val = predictions.get(target)
                        if val is not None and abs(val) > 1e-6:
                            all_zeros = False
                            break
                    
                    if all_zeros:
                        # é¢„æµ‹å¤±è´¥ï¼ˆå…¨0ï¼‰ï¼Œè®°å½•å¤±è´¥ä½†ä¿ç•™Promptå’ŒResponse
                        error_msg = f"Prediction failed: All target properties predicted as zero or None. Response: {result_data['response_data'].get('llm_response', '')[:100]}..."
                        logger.warning(f"Task {task_id}: æ ·æœ¬{sample_idx}é¢„æµ‹å…¨ä¸º0: {error_msg}")
                        state["failed_samples"][sample_idx] = error_msg
                        # ä¸æ›´æ–° iteration_historyï¼Œè¿™æ ·å®ƒä¼šè¢«è§†ä¸ºå¤±è´¥
                    else:
                        # é¢„æµ‹æˆåŠŸï¼Œæ›´æ–°è¿­ä»£ç»“æœå’Œå†å²
                        iteration_predictions[sample_idx] = predictions
                        self._update_iteration_history(state, sample_idx, predictions)

                    # æ›´æ–°è¿›åº¦
                    completed_count += 1
                    self._update_sample_progress(
                        state, current_iter, completed_count,
                        len(samples_to_predict), total_samples
                    )

                except Exception as e:
                    logger.error(
                        f"Task {task_id}: æ ·æœ¬{sample_idx}é¢„æµ‹å¤±è´¥: {e}",
                        exc_info=True
                    )
                    state["failed_samples"][sample_idx] = str(e)
                    completed_count += 1
                    self._update_sample_progress(
                        state, current_iter, completed_count,
                        len(samples_to_predict), total_samples
                    )

        return iteration_predictions

    def _update_iteration_history(
        self,
        state: IterationState,
        sample_idx: int,
        predictions: Dict[str, float]
    ):
        """æ›´æ–°æ ·æœ¬çš„è¿­ä»£å†å²"""
        if sample_idx not in state["iteration_history"]:
            state["iteration_history"][sample_idx] = {
                prop: [] for prop in state["target_properties"]
            }

        for prop in state["target_properties"]:
            state["iteration_history"][sample_idx][prop].append(
                predictions.get(prop, 0.0)
            )

    def _update_sample_progress(
        self,
        state: IterationState,
        current_iter: int,
        completed_count: int,
        total_to_predict: int,
        total_samples: int
    ):
        """æ›´æ–°å•ä¸ªæ ·æœ¬å®Œæˆåçš„è¿›åº¦"""
        progress = len(state["converged_samples"]) / total_samples if total_samples > 0 else 0.0
        self.task_manager.update_task(
            state["task_id"],
            {
                "progress": progress,
                "message": f"ç¬¬{current_iter}è½®: å·²å®Œæˆ{completed_count}/{total_to_predict}ä¸ªæ ·æœ¬ï¼Œå·²æ”¶æ•›{len(state['converged_samples'])}ä¸ª"
            }
        )

    def _update_iteration_progress(
        self,
        state: IterationState,
        current_iter: int,
        predictions_count: int
    ):
        """æ›´æ–°è¿­ä»£å®Œæˆåçš„è¿›åº¦"""
        total_samples = len(state["test_data"])
        completed_samples = len(state["converged_samples"]) + len(state["failed_samples"])
        progress = completed_samples / total_samples if total_samples > 0 else 0.0

        self.task_manager.update_task(
            state["task_id"],
            {
                "progress": progress,
                "message": f"ç¬¬{current_iter}è½®è¿­ä»£å®Œæˆï¼Œå·²æ”¶æ•›{len(state['converged_samples'])}ä¸ªæ ·æœ¬"
            }
        )

        logger.info(
            f"Task {state['task_id']}: ç¬¬{current_iter}è½®è¿­ä»£å®Œæˆï¼Œ"
            f"æˆåŠŸé¢„æµ‹{predictions_count}ä¸ªæ ·æœ¬ï¼Œç»“æœå·²ä¿å­˜"
        )

    def _check_sample_convergence_and_update(
        self,
        state: IterationState,
        sample_idx: int,
        current_iter: int
    ) -> bool:
        """
        æ£€æŸ¥å•ä¸ªæ ·æœ¬çš„æ”¶æ•›æƒ…å†µå¹¶æ›´æ–°çŠ¶æ€

        Returns:
            æ˜¯å¦æ–°æ”¶æ•›
        """
        converged, rel_changes = self.convergence_checker.check_sample_convergence(
            sample_idx,
            state["target_properties"],
            state["iteration_history"][sample_idx]
        )

        if converged:
            state["converged_samples"].add(sample_idx)
            logger.info(
                f"Task {state['task_id']}: æ ·æœ¬{sample_idx}åœ¨ç¬¬{current_iter}è½®æ”¶æ•›ï¼Œ"
                f"ç›¸å¯¹å˜åŒ–ç‡={rel_changes}"
            )
            return True
        return False

    def _node_check_convergence(self, state: IterationState) -> IterationState:
        """
        æ”¶æ•›æ£€æŸ¥èŠ‚ç‚¹ - æ£€æŸ¥æ¯ä¸ªæ ·æœ¬æ˜¯å¦æ”¶æ•›
        """
        task_id = state['task_id']
        current_iter = state["current_iteration"]
        logger.info(f"Task {task_id}: æ£€æŸ¥ç¬¬{current_iter}è½®æ”¶æ•›æƒ…å†µ")

        # åªåœ¨ç¬¬2è½®åŠä»¥åæ£€æŸ¥æ”¶æ•›
        if current_iter < 2:
            logger.info(f"Task {task_id}: ç¬¬1è½®ä¸æ£€æŸ¥æ”¶æ•›")
            # å¢åŠ è¿­ä»£è®¡æ•°å™¨ï¼ˆå³ä½¿ä¸æ£€æŸ¥æ”¶æ•›ä¹Ÿè¦å¢åŠ ï¼‰
            state["current_iteration"] += 1
            return state

        # æ›´æ–°æ”¶æ•›æ£€æŸ¥å™¨çš„é˜ˆå€¼
        self.convergence_checker.threshold = state["convergence_threshold"]

        # æ£€æŸ¥æ‰€æœ‰æ ·æœ¬çš„æ”¶æ•›æƒ…å†µ
        newly_converged_count = 0
        for sample_idx in state["iteration_history"].keys():
            # è·³è¿‡å·²æ”¶æ•›æˆ–å¤±è´¥çš„æ ·æœ¬
            if sample_idx in state["converged_samples"] or sample_idx in state["failed_samples"]:
                continue

            if self._check_sample_convergence_and_update(state, sample_idx, current_iter):
                newly_converged_count += 1

        logger.info(
            f"Task {task_id}: ç¬¬{current_iter}è½®æ–°å¢æ”¶æ•›{newly_converged_count}ä¸ªæ ·æœ¬ï¼Œ"
            f"ç´¯è®¡æ”¶æ•›{len(state['converged_samples'])}ä¸ªæ ·æœ¬"
        )

        # å¢åŠ è¿­ä»£è®¡æ•°å™¨
        state["current_iteration"] += 1

        return state

    def _node_handle_failure(self, state: IterationState) -> IterationState:
        """
        å¤±è´¥å¤„ç†èŠ‚ç‚¹ - è®°å½•å¤±è´¥æ ·æœ¬ï¼Œä¸ä¸­æ–­æ•´ä½“æµç¨‹
        """
        task_id = state['task_id']
        logger.info(
            f"Task {task_id}: å¤„ç†å¤±è´¥æ ·æœ¬ï¼Œ"
            f"å¤±è´¥æ•°é‡={len(state['failed_samples'])}"
        )

        # å¤±è´¥æ ·æœ¬å·²åœ¨predict_iterationèŠ‚ç‚¹ä¸­è®°å½•ï¼Œè¿™é‡Œåªè®°å½•æ—¥å¿—
        for sample_idx, error_msg in state["failed_samples"].items():
            logger.warning(f"Task {task_id}: æ ·æœ¬{sample_idx}å¤±è´¥: {error_msg}")

        return state

    def _build_global_info(self, state: IterationState) -> Dict[str, Any]:
        """æ„å»ºå…¨å±€ä¿¡æ¯"""
        return {
            "task_id": state["task_id"],
            "total_iterations": state["current_iteration"],
            "max_iterations": state["max_iterations"],
            "convergence_threshold": state["convergence_threshold"],
            "early_stopped": state["early_stop"] and state["current_iteration"] < state["max_iterations"],
            "total_samples": len(state["test_data"]),
            "converged_samples": len(state["converged_samples"]),
            "failed_samples": len(state["failed_samples"])
        }

    def _calculate_relative_changes(self, iterations: List[float]) -> List[Optional[float]]:
        """è®¡ç®—ç›¸å¯¹å˜åŒ–ç‡"""
        relative_changes = [None]  # ç¬¬1è½®æ²¡æœ‰å˜åŒ–ç‡
        for i in range(1, len(iterations)):
            if abs(iterations[i-1]) > 1e-6:
                rel_change = abs(iterations[i] - iterations[i-1]) / abs(iterations[i-1])
            else:
                rel_change = abs(iterations[i] - iterations[i-1])
            relative_changes.append(rel_change)
        return relative_changes

    def _get_convergence_status(
        self,
        sample_idx: int,
        state: IterationState,
        iterations: List[float]
    ) -> tuple:
        """
        è·å–æ”¶æ•›çŠ¶æ€

        Returns:
            (convergence_status, converged_at)
        """
        if sample_idx in state["converged_samples"]:
            return "converged", len(iterations)
        elif sample_idx in state["failed_samples"]:
            return "failed", None
        else:
            return "not_converged", None

    def _build_sample_info(
        self,
        sample_idx: int,
        history: Dict[str, List[float]],
        state: IterationState
    ) -> Dict[str, Any]:
        """æ„å»ºå•ä¸ªæ ·æœ¬çš„ä¿¡æ¯"""
        sample_info = {
            "sample_index": sample_idx,
            "targets": {}
        }

        for target_prop in state["target_properties"]:
            iterations = history.get(target_prop, [])
            relative_changes = self._calculate_relative_changes(iterations)
            convergence_status, converged_at = self._get_convergence_status(
                sample_idx, state, iterations
            )

            sample_info["targets"][target_prop] = {
                "iterations": iterations,
                "converged_at_iteration": converged_at,
                "convergence_status": convergence_status,
                "relative_changes": relative_changes
            }

        return sample_info

    def _build_iteration_history_json(self, state: IterationState) -> Dict[str, Any]:
        """æ„å»ºè¿­ä»£å†å²JSON"""
        iteration_history_json = {
            "global_info": self._build_global_info(state),
            "samples": {}
        }

        # æ·»åŠ æ¯ä¸ªæ ·æœ¬çš„è¿­ä»£å†å²
        for sample_idx, history in state["iteration_history"].items():
            sample_info = self._build_sample_info(sample_idx, history, state)
            iteration_history_json["samples"][f"sample_{sample_idx}"] = sample_info

        return iteration_history_json

    def _node_save_results(self, state: IterationState) -> IterationState:
        """
        ä¿å­˜ç»“æœèŠ‚ç‚¹ - ä¿å­˜è¿­ä»£å†å²å’Œæœ€ç»ˆç»“æœåˆ°æ•°æ®åº“å’Œæ–‡ä»¶ç³»ç»Ÿ
        """
        task_id = state['task_id']
        logger.info(f"Task {task_id}: ä¿å­˜è¿­ä»£é¢„æµ‹ç»“æœ")

        # æ„å»ºè¿­ä»£å†å²JSON
        iteration_history_json = self._build_iteration_history_json(state)

        # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ç³»ç»Ÿ
        self._save_results_to_filesystem(state, iteration_history_json)

        # æ›´æ–°ä»»åŠ¡æ•°æ®åº“
        self.task_db.update_task(
            task_id,
            {
                "current_iteration": state["current_iteration"],
                "iteration_history": iteration_history_json,
                "failed_samples": state["failed_samples"],
                "result_id": task_id
            }
        )

        logger.info(
            f"Task {task_id}: è¿­ä»£é¢„æµ‹å®Œæˆï¼Œ"
            f"æ€»è¿­ä»£æ¬¡æ•°={state['current_iteration']}, "
            f"æ”¶æ•›æ ·æœ¬={len(state['converged_samples'])}, "
            f"å¤±è´¥æ ·æœ¬={len(state['failed_samples'])}"
        )

        return state

    def _save_results_to_filesystem(self, state: IterationState, iteration_history_json: Dict[str, Any]):
        """
        ä¿å­˜è¿­ä»£é¢„æµ‹ç»“æœåˆ°æ–‡ä»¶ç³»ç»Ÿ

        Args:
            state: è¿­ä»£çŠ¶æ€
            iteration_history_json: è¿­ä»£å†å²JSON
        """
        task_id = state["task_id"]

        try:
            # åˆ›å»ºç»“æœç›®å½•
            result_dir = RESULTS_DIR / task_id
            result_dir.mkdir(parents=True, exist_ok=True)

            # 1. ä¿å­˜è¿­ä»£å†å²JSON
            iteration_history_file = result_dir / "iteration_history.json"
            iteration_history_content = json.dumps(iteration_history_json, ensure_ascii=False, indent=2)
            if safe_write_file(iteration_history_file, iteration_history_content):
                logger.info(f"Task {task_id}: å·²ä¿å­˜è¿­ä»£å†å²åˆ° iteration_history.json")
            else:
                logger.error(f"Task {task_id}: ä¿å­˜è¿­ä»£å†å²å¤±è´¥")

            # 2. æ„å»ºé¢„æµ‹ç»“æœCSVï¼ˆä¸ºæ¯ä¸ªç›®æ ‡å±æ€§åˆ›å»ºå¤šä¸ªé¢„æµ‹åˆ—ï¼‰
            # æ³¨æ„ï¼šä¿ç•™æ‰€æœ‰åŸå§‹æ•°æ®åˆ—ï¼Œç¡®ä¿æ ¼å¼ä¸ RAG é¢„æµ‹æœåŠ¡ä¸€è‡´
            predictions_data = []
            
            # ç¡®å®šè¦è¾“å‡ºçš„æ ·æœ¬ç´¢å¼•èŒƒå›´
            sample_size = state.get("sample_size")
            total_samples = len(state["test_data"])
            
            if sample_size is not None and sample_size > 0:
                indices_to_export = range(min(sample_size, total_samples))
            else:
                indices_to_export = range(total_samples)

            for sample_idx in indices_to_export:
                test_sample = state["test_data"][sample_idx]
                
                # å¤åˆ¶æ‰€æœ‰åŸå§‹åˆ—ï¼ˆåŒ…æ‹¬å…ƒç´ åˆ—ã€å·¥è‰ºåˆ—ç­‰ï¼‰
                row = test_sample.copy()

                # ç¡®ä¿ sample_index åˆ—å­˜åœ¨
                row["sample_index"] = sample_idx

                # ä¸ºæ¯ä¸ªç›®æ ‡å±æ€§æ·»åŠ æ¯è½®è¿­ä»£çš„é¢„æµ‹å€¼
                if sample_idx in state["iteration_history"]:
                    history = state["iteration_history"][sample_idx]
                    for target_prop in state["target_properties"]:
                        iterations = history.get(target_prop, [])

                        # ä¸ºæ¯è½®è¿­ä»£åˆ›å»ºä¸€ä¸ªé¢„æµ‹åˆ—
                        for iter_num in range(1, state["max_iterations"] + 1):
                            col_name = f"{target_prop}_predicted_Iteration_{iter_num}"
                            if iter_num <= len(iterations):
                                row[col_name] = iterations[iter_num - 1]
                            else:
                                row[col_name] = None  # è¯¥æ ·æœ¬åœ¨è¿™è½®æ²¡æœ‰é¢„æµ‹

                    # æ·»åŠ æ”¶æ•›ä¿¡æ¯
                    if sample_idx in state["converged_samples"]:
                        row["convergence_status"] = "converged"
                        # æ‰¾åˆ°æ”¶æ•›çš„è½®æ¬¡ï¼ˆæœ€åä¸€æ¬¡é¢„æµ‹çš„è½®æ¬¡ï¼‰
                        row["converged_at_iteration"] = len(iterations)
                    elif sample_idx in state["failed_samples"]:
                        row["convergence_status"] = "failed"
                        row["converged_at_iteration"] = None
                    else:
                        row["convergence_status"] = "not_converged"
                        row["converged_at_iteration"] = None
                else:
                    # æ ·æœ¬æ²¡æœ‰é¢„æµ‹å†å²ï¼ˆå¯èƒ½å¤±è´¥äº†æˆ–è€…è¢«è·³è¿‡ï¼‰
                    for target_prop in state["target_properties"]:
                        for iter_num in range(1, state["max_iterations"] + 1):
                            col_name = f"{target_prop}_predicted_Iteration_{iter_num}"
                            row[col_name] = None
                    row["convergence_status"] = "failed"
                    row["converged_at_iteration"] = None

                predictions_data.append(row)

            # ä¿å­˜predictions.csvï¼ˆä¿ç•™æ‰€æœ‰åŸå§‹åˆ—ï¼Œå¹¶è°ƒæ•´åˆ—é¡ºåºï¼‰
            predictions_df = pd.DataFrame(predictions_data)

            # è°ƒæ•´åˆ—é¡ºåºï¼šsample_index, IDï¼ˆå¦‚æœæœ‰ï¼‰, åŸå§‹æ•°æ®åˆ—, é¢„æµ‹åˆ—, æ”¶æ•›çŠ¶æ€åˆ—
            # 1. ç¡®å®šåˆ—é¡ºåº
            ordered_columns = []

            # é¦–å…ˆæ·»åŠ  sample_index
            if "sample_index" in predictions_df.columns:
                ordered_columns.append("sample_index")

            # ç„¶åæ·»åŠ  IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "ID" in predictions_df.columns:
                ordered_columns.append("ID")

            # æ·»åŠ æ‰€æœ‰åŸå§‹æ•°æ®åˆ—ï¼ˆæ’é™¤ sample_index, ID, composition, sample_text, é¢„æµ‹åˆ—, æ”¶æ•›çŠ¶æ€åˆ—ï¼‰
            exclude_cols = {"sample_index", "ID", "composition", "sample_text", "convergence_status", "converged_at_iteration"}
            for col in predictions_df.columns:
                if col not in exclude_cols and not col.endswith("_predicted_Iteration_1") and \
                   not col.endswith("_predicted_Iteration_2") and not col.endswith("_predicted_Iteration_3") and \
                   col not in ordered_columns:
                    ordered_columns.append(col)

            # æ·»åŠ  compositionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "composition" in predictions_df.columns:
                ordered_columns.append("composition")

            # æ·»åŠ é¢„æµ‹åˆ—ï¼ˆæŒ‰è¿­ä»£è½®æ¬¡æ’åºï¼‰
            prediction_cols = [col for col in predictions_df.columns if "_predicted_Iteration_" in col]
            ordered_columns.extend(sorted(prediction_cols))

            # æœ€åæ·»åŠ æ”¶æ•›çŠ¶æ€åˆ—
            if "convergence_status" in predictions_df.columns:
                ordered_columns.append("convergence_status")
            if "converged_at_iteration" in predictions_df.columns:
                ordered_columns.append("converged_at_iteration")

            # 2. é‡æ–°æ’åˆ—åˆ—é¡ºåº
            predictions_df = predictions_df[ordered_columns]

            predictions_file = result_dir / "predictions.csv"
            predictions_df.to_csv(predictions_file, index=False, encoding='utf-8')
            logger.info(f"Task {task_id}: å·²ä¿å­˜é¢„æµ‹ç»“æœåˆ° predictions.csv ({len(predictions_df)} ä¸ªæ ·æœ¬)")

            # 3. è®¡ç®—å¹¶ä¿å­˜è¯„ä¼°æŒ‡æ ‡
            metrics = self._calculate_iterative_metrics(predictions_df, state["target_properties"])
            metrics_file = result_dir / "metrics.json"
            metrics_content = json.dumps(metrics, ensure_ascii=False, indent=2)
            if safe_write_file(metrics_file, metrics_content):
                logger.info(f"Task {task_id}: å·²ä¿å­˜è¯„ä¼°æŒ‡æ ‡åˆ° metrics.json")
            else:
                logger.error(f"Task {task_id}: ä¿å­˜è¯„ä¼°æŒ‡æ ‡å¤±è´¥")

            # 4. ä¿å­˜ä»»åŠ¡é…ç½®
            # è·å–ä»»åŠ¡çŠ¶æ€ä¿¡æ¯ï¼ˆä½¿ç”¨ get_task_status è·å–åŸå§‹æ•°æ®ï¼ŒåŒ…å« request_dataï¼‰
            task_info = self.task_manager.get_task_status(task_id) or {}
            
            # ä» TaskManager è·å–åŸå§‹ request_dataï¼Œé¿å…ä» config é‡æ„å¯¼è‡´ä¸¢å¤±å­—æ®µ
            request_data = task_info.get("request_data", {})
            if not request_data:
                request_data = {
                    "filename": Path(state["config"].get("data_path", "")).name if state["config"].get("data_path") else "",
                    "file_path": state["config"].get("data_path", ""),
                    "config": state["config"],
                    "dataset_id": state["config"].get("dataset_id", ""),
                    "file_id": state["config"].get("file_id", ""),
                    "note": state["config"].get("note", "")
                }

            task_config = {
                "task_id": task_id,
                "status": task_info.get("status", "completed"),
                "progress": task_info.get("progress", 1.0),
                "message": task_info.get("message", "é¢„æµ‹å®Œæˆ"),
                "created_at": state["start_time"].isoformat(),
                "updated_at": datetime.now().isoformat(),
                "request_data": request_data,
                "total_rows": len(predictions_data),
                "valid_rows": len(predictions_data),
                "note": state["config"].get("note", ""),
                "total_iterations": state["current_iteration"],
                "max_iterations": state["max_iterations"],
                "convergence_threshold": state["convergence_threshold"],
                "enable_iteration": True,
                "early_stop": state["early_stop"],
                "max_workers": state["max_workers"]
            }
            task_config_file = result_dir / "task_config.json"
            task_config_content = json.dumps(task_config, ensure_ascii=False, indent=2)
            if safe_write_file(task_config_file, task_config_content):
                logger.info(f"Task {task_id}: å·²ä¿å­˜ä»»åŠ¡é…ç½®åˆ° task_config.json")
            else:
                logger.error(f"Task {task_id}: ä¿å­˜ä»»åŠ¡é…ç½®å¤±è´¥")

            # 5. ä¿å­˜æµ‹è¯•é›†
            test_df = pd.DataFrame(state["test_data"])
            test_set_file = result_dir / "test_set.csv"
            test_df.to_csv(test_set_file, index=False, encoding='utf-8')
            logger.info(f"Task {task_id}: å·²ä¿å­˜æµ‹è¯•é›†åˆ° test_set.csv")

            logger.info(f"Task {task_id}: æ‰€æœ‰ç»“æœæ–‡ä»¶å·²ä¿å­˜åˆ° {result_dir}")

        except Exception as e:
            logger.error(f"Task {task_id}: ä¿å­˜ç»“æœåˆ°æ–‡ä»¶ç³»ç»Ÿå¤±è´¥: {e}", exc_info=True)

    def _calculate_iterative_metrics(self, df: pd.DataFrame, target_properties: List[str]) -> Dict[str, Any]:
        """
        è®¡ç®—è¿­ä»£é¢„æµ‹çš„è¯„ä¼°æŒ‡æ ‡ï¼ˆä¸ºæ¯è½®è¿­ä»£è®¡ç®—æŒ‡æ ‡ï¼‰

        Args:
            df: é¢„æµ‹ç»“æœDataFrame
            target_properties: ç›®æ ‡å±æ€§åˆ—è¡¨

        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
        import numpy as np

        metrics = {}

        # æ£€æŸ¥çœŸå®å€¼åˆ—æ˜¯å¦å­˜åœ¨
        for target_prop in target_properties:
            if target_prop not in df.columns:
                logger.debug(f"è·³è¿‡æŒ‡æ ‡è®¡ç®—ï¼šçœŸå®å€¼åˆ— '{target_prop}' ä¸å­˜åœ¨")
                continue

            # ä¸ºæ¯ä¸ªç›®æ ‡å±æ€§è®¡ç®—æ¯è½®è¿­ä»£çš„æŒ‡æ ‡
            target_metrics = {}

            # æ‰¾å‡ºæ‰€æœ‰è¿­ä»£åˆ—
            iteration_cols = [col for col in df.columns if col.startswith(f"{target_prop}_predicted_Iteration_")]

            for pred_col in iteration_cols:
                # æå–è¿­ä»£è½®æ¬¡
                iter_num = pred_col.split("_")[-1]

                # è®¡ç®—è¯¥è½®è¿­ä»£çš„æŒ‡æ ‡
                iter_metrics = self._calculate_single_target_metrics(
                    df, target_prop, pred_col,
                    mean_absolute_error, mean_squared_error, r2_score, np
                )

                target_metrics[f"Iteration_{iter_num}"] = iter_metrics

            if target_metrics:
                metrics[target_prop] = target_metrics

        return metrics

    def _calculate_single_target_metrics(
        self,
        df: pd.DataFrame,
        target_prop: str,
        pred_col: str,
        mae_func,
        mse_func,
        r2_func,
        np_module
    ) -> Dict[str, Any]:
        """
        è®¡ç®—å•ä¸ªç›®æ ‡å±æ€§çš„è¯„ä¼°æŒ‡æ ‡

        Returns:
            æŒ‡æ ‡å­—å…¸
        """
        # è¿‡æ»¤æ‰ç¼ºå¤±å€¼
        valid_mask = df[target_prop].notna() & df[pred_col].notna()
        y_true = df.loc[valid_mask, target_prop].values
        y_pred = df.loc[valid_mask, pred_col].values

        if len(y_true) > 0:
            mae = mae_func(y_true, y_pred)
            rmse = np_module.sqrt(mse_func(y_true, y_pred))
            r2 = r2_func(y_true, y_pred) if len(y_true) > 1 else 0.0

            return {
                "MAE": float(mae),
                "RMSE": float(rmse),
                "R2": float(r2),
                "sample_count": int(len(y_true))
            }
        else:
            return {
                "MAE": None,
                "RMSE": None,
                "R2": None,
                "sample_count": 0
            }

    def _build_iteration_global_info(self, state: IterationState, current_iter: int) -> Dict[str, Any]:
        """æ„å»ºè¿­ä»£å…¨å±€ä¿¡æ¯"""
        return {
            "task_id": state["task_id"],
            "current_iteration": current_iter,
            "max_iterations": state["max_iterations"],
            "convergence_threshold": state["convergence_threshold"],
            "total_samples": len(state["test_data"]),
            "converged_samples": len(state["converged_samples"]),
            "failed_samples": len(state["failed_samples"])
        }

    def _get_iteration_convergence_status(
        self,
        sample_idx: int,
        state: IterationState,
        iterations: List[float]
    ) -> tuple:
        """
        è·å–è¿­ä»£ä¸­çš„æ”¶æ•›çŠ¶æ€

        Returns:
            (convergence_status, converged_at)
        """
        if sample_idx in state["converged_samples"]:
            return "converged", len(iterations)
        elif sample_idx in state["failed_samples"]:
            return "failed", None
        else:
            return "in_progress", None

    def _build_iteration_sample_info(
        self,
        sample_idx: int,
        history: Dict[str, List[float]],
        state: IterationState
    ) -> Dict[str, Any]:
        """æ„å»ºè¿­ä»£ä¸­å•ä¸ªæ ·æœ¬çš„ä¿¡æ¯"""
        sample_info = {
            "sample_index": sample_idx,
            "targets": {}
        }

        for target_prop in state["target_properties"]:
            iterations = history.get(target_prop, [])
            relative_changes = self._calculate_relative_changes(iterations)
            convergence_status, converged_at = self._get_iteration_convergence_status(
                sample_idx, state, iterations
            )

            sample_info["targets"][target_prop] = {
                "iterations": iterations,
                "converged_at_iteration": converged_at,
                "convergence_status": convergence_status,
                "relative_changes": relative_changes
            }

        return sample_info

    def _build_iteration_history_for_current(
        self,
        state: IterationState,
        current_iter: int
    ) -> Dict[str, Any]:
        """æ„å»ºå½“å‰è¿­ä»£çš„å†å²JSON"""
        iteration_history_json = {
            "global_info": self._build_iteration_global_info(state, current_iter),
            "samples": {}
        }

        # æ·»åŠ æ¯ä¸ªæ ·æœ¬çš„è¿­ä»£å†å²
        for sample_idx, history in state["iteration_history"].items():
            sample_info = self._build_iteration_sample_info(sample_idx, history, state)
            iteration_history_json["samples"][f"sample_{sample_idx}"] = sample_info

        return iteration_history_json

    def _save_iteration_results(self, state: IterationState, current_iter: int):
        """
        ä¿å­˜å½“å‰è¿­ä»£çš„ç»“æœåˆ°æ•°æ®åº“å’Œæ–‡ä»¶ç³»ç»Ÿ

        Args:
            state: å½“å‰çŠ¶æ€
            current_iter: å½“å‰è¿­ä»£è½®æ¬¡
        """
        try:
            # æ„å»ºè¿­ä»£å†å²JSON
            iteration_history_json = self._build_iteration_history_for_current(state, current_iter)

            # æ›´æ–°ä»»åŠ¡æ•°æ®åº“
            self.task_db.update_task(
                state["task_id"],
                {
                    "current_iteration": current_iter,
                    "iteration_history": iteration_history_json,
                    "failed_samples": state["failed_samples"]
                }
            )

            logger.info(f"Task {state['task_id']}: ç¬¬{current_iter}è½®ç»“æœå·²ä¿å­˜åˆ°æ•°æ®åº“")

            # å¢é‡ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ
            self._save_incremental_results_to_filesystem(state, iteration_history_json, current_iter)

        except Exception as e:
            logger.error(
                f"Task {state['task_id']}: ä¿å­˜ç¬¬{current_iter}è½®ç»“æœå¤±è´¥: {e}",
                exc_info=True
            )

    def _save_incremental_results_to_filesystem(
        self,
        state: IterationState,
        iteration_history_json: Dict[str, Any],
        current_iter: int
    ):
        """
        å¢é‡ä¿å­˜è¿­ä»£ç»“æœåˆ°æ–‡ä»¶ç³»ç»Ÿï¼ˆæ¯è½®è¿­ä»£åè°ƒç”¨ï¼‰

        Args:
            state: è¿­ä»£çŠ¶æ€
            iteration_history_json: è¿­ä»£å†å²JSONï¼ˆæˆªè‡³å½“å‰è½®æ¬¡ï¼‰
            current_iter: å½“å‰è¿­ä»£è½®æ¬¡
        """
        task_id = state["task_id"]

        try:
            # åˆ›å»ºç»“æœç›®å½•
            result_dir = RESULTS_DIR / task_id
            result_dir.mkdir(parents=True, exist_ok=True)

            # 1. ä¿å­˜è¿­ä»£å†å²JSONï¼ˆå¢é‡æ›´æ–°ï¼‰
            iteration_history_file = result_dir / "iteration_history.json"
            iteration_history_content = json.dumps(iteration_history_json, ensure_ascii=False, indent=2)
            if safe_write_file(iteration_history_file, iteration_history_content):
                logger.info(f"Task {task_id}: ç¬¬{current_iter}è½® - å·²æ›´æ–° iteration_history.json")
            else:
                logger.error(f"Task {task_id}: ç¬¬{current_iter}è½® - æ›´æ–° iteration_history.json å¤±è´¥")

            # 2. æ„å»ºå¹¶ä¿å­˜å½“å‰é¢„æµ‹ç»“æœCSVï¼ˆå¢é‡æ›´æ–°ï¼Œä¸ºæ¯ä¸ªç›®æ ‡å±æ€§åˆ›å»ºå¤šä¸ªé¢„æµ‹åˆ—ï¼‰
            predictions_data = []
            valid_rows_count = 0
            
            # ç¡®å®šè¦è¾“å‡ºçš„æ ·æœ¬ç´¢å¼•èŒƒå›´
            sample_size = state.get("sample_size")
            total_samples = len(state["test_data"])
            
            if sample_size is not None and sample_size > 0:
                indices_to_export = range(min(sample_size, total_samples))
            else:
                indices_to_export = range(total_samples)
                
            for sample_idx in indices_to_export:
                test_sample = state["test_data"][sample_idx]
                
                # ä¸å†è·³è¿‡æœªå¤„ç†çš„æ ·æœ¬ï¼Œä»¥ç¡®ä¿è¾“å‡ºè¡Œæ•°ä¸ sample_size ä¸€è‡´
                # if sample_idx not in state["iteration_history"] and sample_idx not in state["failed_samples"]:
                #     continue

                row = test_sample.copy()
                row["sample_index"] = sample_idx
                
                is_sample_valid = False

                # ä¸ºæ¯ä¸ªç›®æ ‡å±æ€§æ·»åŠ æ¯è½®è¿­ä»£çš„é¢„æµ‹å€¼
                if sample_idx in state["iteration_history"]:
                    history = state["iteration_history"][sample_idx]
                    
                    # æ£€æŸ¥è¯¥æ ·æœ¬æ˜¯å¦æœ‰æ•ˆï¼ˆæ‰€æœ‰ç›®æ ‡å±æ€§çš„æœ€æ–°é¢„æµ‹å€¼éƒ½ä¸ä¸º0ä¸”ä¸ä¸ºç©ºï¼‰
                    # åªè¦æœ‰ä¸€ä¸ªç›®æ ‡å±æ€§æœ‰æœ‰æ•ˆé¢„æµ‹ï¼Œæˆ‘ä»¬æš‚ä¸”è®¤ä¸ºæœ‰æ•ˆï¼Œæˆ–è€…ä¸¥æ ¼ä¸€ç‚¹è¦æ±‚æ‰€æœ‰ï¼Ÿ
                    # æ ¹æ®ç”¨æˆ·è¦æ±‚ "é¢„æµ‹å€¼ä¸ä¸ºé›¶æˆ–ç©ºçš„è¡Œæ•°"ï¼Œé€šå¸¸æŒ‡æœ‰æ•ˆè¾“å‡ºã€‚
                    # è¿™é‡Œæ£€æŸ¥æ‰€æœ‰ç›®æ ‡å±æ€§çš„æœ€åä¸€ä¸ªå€¼
                    all_targets_valid = True
                    for target_prop in state["target_properties"]:
                        vals = history.get(target_prop, [])
                        if not vals or vals[-1] is None or abs(vals[-1]) < 1e-6:
                            all_targets_valid = False
                            break
                    if all_targets_valid:
                        is_sample_valid = True

                    for target_prop in state["target_properties"]:
                        iterations = history.get(target_prop, [])

                        # ä¸ºæ¯è½®è¿­ä»£åˆ›å»ºä¸€ä¸ªé¢„æµ‹åˆ—ï¼ˆæˆªè‡³å½“å‰è½®æ¬¡ï¼‰
                        for iter_num in range(1, state["max_iterations"] + 1):
                            col_name = f"{target_prop}_predicted_Iteration_{iter_num}"
                            if iter_num <= len(iterations):
                                row[col_name] = iterations[iter_num - 1]
                            else:
                                row[col_name] = None  # è¯¥æ ·æœ¬åœ¨è¿™è½®è¿˜æ²¡æœ‰é¢„æµ‹

                    # æ·»åŠ æ”¶æ•›ä¿¡æ¯
                    if sample_idx in state["converged_samples"]:
                        row["convergence_status"] = "converged"
                        row["converged_at_iteration"] = len(iterations)
                    elif sample_idx in state["failed_samples"]:
                        row["convergence_status"] = "failed"
                        row["converged_at_iteration"] = None
                    else:
                        row["convergence_status"] = "in_progress"
                        row["converged_at_iteration"] = None
                else:
                    # æ ·æœ¬å¤±è´¥äº†ï¼ˆåœ¨failed_samplesä¸­ä½†ä¸åœ¨iteration_historyä¸­ï¼‰
                    for target_prop in state["target_properties"]:
                        for iter_num in range(1, state["max_iterations"] + 1):
                            col_name = f"{target_prop}_predicted_Iteration_{iter_num}"
                            row[col_name] = None
                    row["convergence_status"] = "failed"
                    row["converged_at_iteration"] = None
                
                if is_sample_valid:
                    valid_rows_count += 1

                predictions_data.append(row)

            # ä¿å­˜predictions.csvï¼ˆè°ƒæ•´åˆ—é¡ºåºï¼‰
            predictions_df = pd.DataFrame(predictions_data)

            # è°ƒæ•´åˆ—é¡ºåºï¼šsample_index, IDï¼ˆå¦‚æœæœ‰ï¼‰, åŸå§‹æ•°æ®åˆ—, é¢„æµ‹åˆ—, æ”¶æ•›çŠ¶æ€åˆ—
            ordered_columns = []

            # é¦–å…ˆæ·»åŠ  sample_index
            if "sample_index" in predictions_df.columns:
                ordered_columns.append("sample_index")

            # ç„¶åæ·»åŠ  IDï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "ID" in predictions_df.columns:
                ordered_columns.append("ID")

            # æ·»åŠ æ‰€æœ‰åŸå§‹æ•°æ®åˆ—ï¼ˆæ’é™¤ sample_index, ID, composition, sample_text, é¢„æµ‹åˆ—, æ”¶æ•›çŠ¶æ€åˆ—ï¼‰
            exclude_cols = {"sample_index", "ID", "composition", "sample_text", "convergence_status", "converged_at_iteration"}
            for col in predictions_df.columns:
                if col not in exclude_cols and not col.endswith("_predicted_Iteration_1") and \
                   not col.endswith("_predicted_Iteration_2") and not col.endswith("_predicted_Iteration_3") and \
                   col not in ordered_columns:
                    ordered_columns.append(col)

            # æ·»åŠ  compositionï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if "composition" in predictions_df.columns:
                ordered_columns.append("composition")

            # æ·»åŠ é¢„æµ‹åˆ—ï¼ˆæŒ‰è¿­ä»£è½®æ¬¡æ’åºï¼‰
            prediction_cols = [col for col in predictions_df.columns if "_predicted_Iteration_" in col]
            ordered_columns.extend(sorted(prediction_cols))

            # æœ€åæ·»åŠ æ”¶æ•›çŠ¶æ€åˆ—
            if "convergence_status" in predictions_df.columns:
                ordered_columns.append("convergence_status")
            if "converged_at_iteration" in predictions_df.columns:
                ordered_columns.append("converged_at_iteration")

            # é‡æ–°æ’åˆ—åˆ—é¡ºåº
            predictions_df = predictions_df[ordered_columns]

            predictions_file = result_dir / "predictions.csv"
            predictions_df.to_csv(predictions_file, index=False, encoding='utf-8')
            logger.info(f"Task {task_id}: ç¬¬{current_iter}è½® - å·²æ›´æ–° predictions.csv ({len(predictions_df)} ä¸ªæ ·æœ¬)")

            # 3. è®¡ç®—å¹¶ä¿å­˜è¯„ä¼°æŒ‡æ ‡ï¼ˆå¢é‡æ›´æ–°ï¼‰
            metrics = self._calculate_iterative_metrics(predictions_df, state["target_properties"])
            metrics["current_iteration"] = current_iter
            metrics["max_iterations"] = state["max_iterations"]
            metrics["converged_samples"] = len(state["converged_samples"])
            metrics["failed_samples"] = len(state["failed_samples"])
            metrics["in_progress_samples"] = len(state["test_data"]) - len(state["converged_samples"]) - len(state["failed_samples"])

            metrics_file = result_dir / "metrics.json"
            metrics_content = json.dumps(metrics, ensure_ascii=False, indent=2)
            if safe_write_file(metrics_file, metrics_content):
                logger.info(f"Task {task_id}: ç¬¬{current_iter}è½® - å·²æ›´æ–° metrics.json")
            else:
                logger.error(f"Task {task_id}: ç¬¬{current_iter}è½® - æ›´æ–° metrics.json å¤±è´¥")

            # 4. ä¿å­˜ä»»åŠ¡é…ç½®ï¼ˆæ¯è½®æ›´æ–°ï¼Œå› ä¸ºtotal_rowså’Œvalid_rowsä¼šå˜åŒ–ï¼‰
            task_config_file = result_dir / "task_config.json"
            # æ€»æ˜¯æ›´æ–° task_config.json
            # è·å–ä»»åŠ¡çŠ¶æ€ä¿¡æ¯ï¼ˆä½¿ç”¨ get_task_status è·å–åŸå§‹æ•°æ®ï¼ŒåŒ…å« request_dataï¼‰
            task_info = self.task_manager.get_task_status(task_id) or {}

            # ä» TaskManager è·å–åŸå§‹ request_data
            request_data = task_info.get("request_data", {})
            if not request_data:
                request_data = {
                    "filename": Path(state["config"].get("data_path", "")).name if state["config"].get("data_path") else "",
                    "file_path": state["config"].get("data_path", ""),
                    "config": state["config"],
                    "dataset_id": state["config"].get("dataset_id", ""),
                    "file_id": state["config"].get("file_id", ""),
                    "note": state["config"].get("note", "")
                }

            task_config = {
                "task_id": task_id,
                "status": task_info.get("status", "running"),
                "progress": task_info.get("progress", 0.0),
                "message": task_info.get("message", ""),
                "created_at": state["start_time"].isoformat(),
                "updated_at": datetime.now().isoformat(),
                "request_data": request_data,
                "total_rows": len(predictions_data),
                "valid_rows": valid_rows_count,
                "note": state["config"].get("note", ""),
                "total_iterations": state["current_iteration"],
                "max_iterations": state["max_iterations"],
                "convergence_threshold": state["convergence_threshold"],
                "enable_iteration": True,
                "early_stop": state["early_stop"],
                "max_workers": state["max_workers"]
            }
            task_config_content = json.dumps(task_config, ensure_ascii=False, indent=2)
            if safe_write_file(task_config_file, task_config_content):
                logger.info(f"Task {task_id}: å·²æ›´æ–° task_config.json (rows={len(predictions_data)}, valid={valid_rows_count})")
            else:
                logger.error(f"Task {task_id}: ä¿å­˜ task_config.json å¤±è´¥")

            # 5. ä¿å­˜æµ‹è¯•é›†ï¼ˆåªåœ¨ç¬¬ä¸€è½®ä¿å­˜ï¼‰
            test_set_file = result_dir / "test_set.csv"
            if current_iter == 1 or not test_set_file.exists():
                test_df = pd.DataFrame(state["test_data"])
                test_df.to_csv(test_set_file, index=False, encoding='utf-8')
                logger.info(f"Task {task_id}: å·²ä¿å­˜ test_set.csv")

            # 6. ä¿å­˜ inputs å’Œ outputs æ–‡ä»¶å¤¹ï¼ˆæ¯è½®å¢é‡ä¿å­˜ï¼‰
            self._save_prompts_and_responses(result_dir, state, current_iter)

            # 7. ç”Ÿæˆå¹¶ä¿å­˜ process_details.jsonï¼ˆæ¯è½®å¢é‡æ›´æ–°ï¼‰
            self._save_process_details(result_dir, state, current_iter)

            logger.info(
                f"Task {task_id}: ç¬¬{current_iter}è½®ç»“æœå·²å¢é‡ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿ "
                f"(æ”¶æ•›:{len(state['converged_samples'])}, "
                f"å¤±è´¥:{len(state['failed_samples'])}, "
                f"è¿›è¡Œä¸­:{len(state['test_data']) - len(state['converged_samples']) - len(state['failed_samples'])})"
            )

        except Exception as e:
            logger.error(f"Task {task_id}: ç¬¬{current_iter}è½®å¢é‡ä¿å­˜åˆ°æ–‡ä»¶ç³»ç»Ÿå¤±è´¥: {e}", exc_info=True)

    def _build_sample_detail(
        self,
        sample_idx: int,
        test_sample: Dict[str, Any],
        state: IterationState
    ) -> Dict[str, Any]:
        """
        æ„å»ºå•ä¸ªæ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯ï¼ˆå®Œå…¨ç»§æ‰¿ RAG é¢„æµ‹æœåŠ¡çš„æ ¼å¼ï¼‰

        Returns:
            æ ·æœ¬è¯¦ç»†ä¿¡æ¯å­—å…¸
        """
        # è·å–æ ·æœ¬æ–‡æœ¬å¹¶åº”ç”¨åˆ—åæ˜ å°„
        sample_text = test_sample.get("sample_text", "")
        
        # è·å–åˆ—åæ˜ å°„é…ç½®
        config = state["config"]
        column_name_mapping = None
        if config.get("prompt_template") and "column_name_mapping" in config["prompt_template"]:
            column_name_mapping = config["prompt_template"]["column_name_mapping"]
        else:
            # ä½¿ç”¨é»˜è®¤åˆ—åæ˜ å°„
            from services.prompt_template_manager import PromptTemplateManager
            column_name_mapping = PromptTemplateManager.get_default_column_mapping()
            
        # åº”ç”¨æ˜ å°„
        if sample_text:
            prompt_builder = PromptBuilder(column_name_mapping=column_name_mapping)
            sample_text = prompt_builder._apply_column_name_mapping(sample_text)

        # è·å–çœŸå®å€¼
        true_values = {
            target_prop: test_sample[target_prop]
            for target_prop in state["target_properties"]
            if target_prop in test_sample
        }

        # è·å–è¿­ä»£å†å²å’Œæœ€ç»ˆé¢„æµ‹å€¼
        iteration_history = {}
        predicted_values = {}  # ä½¿ç”¨ predicted_values è€Œä¸æ˜¯ final_predictions

        if sample_idx in state["iteration_history"]:
            history = state["iteration_history"][sample_idx]
            for target_prop in state["target_properties"]:
                iterations = history.get(target_prop, [])
                if iterations:
                    iteration_history[target_prop] = iterations
                    predicted_values[target_prop] = iterations[-1]  # æœ€åä¸€è½®çš„é¢„æµ‹å€¼

        # è·å–ç›¸ä¼¼æ ·æœ¬ä¿¡æ¯ï¼ˆä»ç¬¬ä¸€è½®çš„å“åº”ä¸­è·å–ï¼‰
        similar_samples = []
        if sample_idx in state["responses"] and 1 in state["responses"][sample_idx]:
            first_iter_response = state["responses"][sample_idx][1]
            similar_samples = first_iter_response.get("similar_samples", [])

        # è·å–æœ€åä¸€è½®çš„ prompt å’Œ llm_response
        # prompt ä½¿ç”¨æœ€åä¸€è½®ï¼ˆä¸ predicted_values ä¿æŒä¸€è‡´ï¼‰
        # llm_response ä½¿ç”¨æœ€åä¸€è½®
        prompt = ""
        llm_response = ""

        if sample_idx in state["prompts"]:
            last_iteration = max(state["prompts"][sample_idx].keys()) if state["prompts"][sample_idx] else None
            if last_iteration:
                prompt = state["prompts"][sample_idx][last_iteration]

        # è·å–æœ€åä¸€è½®çš„ llm_response
        if sample_idx in state["responses"]:
            # æ‰¾åˆ°æœ€åä¸€è½®çš„è¿­ä»£
            last_iteration = max(state["responses"][sample_idx].keys()) if state["responses"][sample_idx] else None
            if last_iteration:
                llm_response = state["responses"][sample_idx][last_iteration].get("llm_response", "")

        # æ„å»ºåŸºæœ¬ä¿¡æ¯ï¼ˆä¸ RAG é¢„æµ‹æœåŠ¡æ ¼å¼å®Œå…¨ä¸€è‡´ï¼‰
        detail = {
            "sample_index": sample_idx,
            "sample_text": sample_text,
            "true_values": true_values,
            "predicted_values": predicted_values,  # ä½¿ç”¨ predicted_values è€Œä¸æ˜¯ final_predictions
            "prompt": prompt,  # ç¬¬ä¸€è½®çš„ prompt
            "llm_response": llm_response,  # ç¬¬ä¸€è½®çš„ llm_response
            "confidence": None,  # é»˜è®¤ç½®ä¿¡åº¦
            "similar_samples": similar_samples,  # ç›¸ä¼¼æ ·æœ¬åˆ—è¡¨
            "iteration_history": iteration_history,  # è¿­ä»£å†å²ï¼ˆè¿­ä»£é¢„æµ‹ç‰¹æœ‰ï¼‰
            "predicted_at": datetime.now().isoformat(),  # é¢„æµ‹æ—¶é—´
            "used_default_values": False  # æ˜¯å¦ä½¿ç”¨é»˜è®¤å€¼
        }

        # è·å–æœ€åä¸€è½®çš„ confidence
        if sample_idx in state["responses"]:
            last_iteration = max(state["responses"][sample_idx].keys()) if state["responses"][sample_idx] else None
            if last_iteration:
                detail["confidence"] = state["responses"][sample_idx][last_iteration].get("confidence")

        # æ·»åŠ  ID å­—æ®µï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "ID" in test_sample:
            detail["ID"] = test_sample["ID"]

        # æ·»åŠ æ¯è½®è¿­ä»£çš„è¯¦ç»†ä¿¡æ¯ï¼ˆprompt å’Œ responseï¼‰
        if sample_idx in state["prompts"]:
            iterations_details = []
            for iteration in sorted(state["prompts"][sample_idx].keys()):
                iteration_detail = {
                    "iteration": iteration,
                    "prompt": state["prompts"][sample_idx].get(iteration, ""),
                }

                # æ·»åŠ è¯¥è½®çš„å“åº”ä¿¡æ¯
                if sample_idx in state["responses"] and iteration in state["responses"][sample_idx]:
                    response_data = state["responses"][sample_idx][iteration]
                    iteration_detail["llm_response"] = response_data.get("llm_response", "")
                    iteration_detail["predictions"] = response_data.get("predictions", {})
                    iteration_detail["confidence"] = response_data.get("confidence")  # æ·»åŠ æ¯è½®çš„ç½®ä¿¡åº¦

                iterations_details.append(iteration_detail)

            detail["iterations_details"] = iterations_details

        return detail

    def _save_process_details(
        self,
        result_dir: Path,
        state: IterationState,
        current_iter: int
    ):
        """
        ç”Ÿæˆå¹¶ä¿å­˜ process_details.json æ–‡ä»¶

        Args:
            result_dir: ç»“æœç›®å½•
            state: è¿­ä»£çŠ¶æ€
            current_iter: å½“å‰è¿­ä»£è½®æ¬¡
        """
        task_id = state["task_id"]

        try:
            process_details = []

            # éå†æ‰€æœ‰æµ‹è¯•æ ·æœ¬ï¼ŒåªåŒ…å«è‡³å°‘è¢«é¢„æµ‹è¿‡ä¸€æ¬¡çš„æ ·æœ¬
            for sample_idx, test_sample in enumerate(state["test_data"]):
                if sample_idx not in state["iteration_history"]:
                    continue

                detail = self._build_sample_detail(sample_idx, test_sample, state)
                process_details.append(detail)

            # ä¿å­˜åˆ°æ–‡ä»¶
            process_details_file = result_dir / "process_details.json"
            process_details_content = json.dumps(process_details, ensure_ascii=False, indent=2)
            if safe_write_file(process_details_file, process_details_content):
                logger.info(
                    f"Task {task_id}: ç¬¬{current_iter}è½® - å·²ä¿å­˜ process_details.json "
                    f"({len(process_details)} ä¸ªæ ·æœ¬è®°å½•)"
                )
            else:
                logger.error(f"Task {task_id}: ç¬¬{current_iter}è½® - ä¿å­˜ process_details.json å¤±è´¥")

        except Exception as e:
            logger.error(f"Task {task_id}: ä¿å­˜ process_details.json å¤±è´¥: {e}", exc_info=True)

    def _save_prompts_and_responses(
        self,
        result_dir: Path,
        state: IterationState,
        current_iter: int
    ):
        """
        ä¿å­˜ prompts å’Œ responses åˆ° inputs/ å’Œ outputs/ æ–‡ä»¶å¤¹

        Args:
            result_dir: ç»“æœç›®å½•
            state: è¿­ä»£çŠ¶æ€
            current_iter: å½“å‰è¿­ä»£è½®æ¬¡
        """
        task_id = state["task_id"]

        try:
            # åˆ›å»º inputs å’Œ outputs ç›®å½•
            inputs_dir = result_dir / "inputs"
            outputs_dir = result_dir / "outputs"
            inputs_dir.mkdir(exist_ok=True)
            outputs_dir.mkdir(exist_ok=True)

            # éå†æ‰€æœ‰æ ·æœ¬ï¼ˆä¸ä»…ä»…æ˜¯æœ‰promptçš„æ ·æœ¬ï¼Œä»¥ç¡®ä¿æ–‡ä»¶å®Œæ•´æ€§ï¼‰
            for sample_idx in range(len(state["test_data"])):
                # å¦‚æœæ ·æœ¬æœªè¢«å¤„ç†ï¼ˆä¸åœ¨ iteration_history ä¸”ä¸åœ¨ failed_samples ä¸­ï¼‰ï¼Œåˆ™è·³è¿‡
                if sample_idx not in state["iteration_history"] and sample_idx not in state["failed_samples"]:
                    continue

                # åˆ›å»ºæ ·æœ¬ç›®å½•
                sample_inputs_dir = inputs_dir / f"sample_{sample_idx}"
                sample_outputs_dir = outputs_dir / f"sample_{sample_idx}"
                sample_inputs_dir.mkdir(exist_ok=True)
                sample_outputs_dir.mkdir(exist_ok=True)

                # éå†æ‰€æœ‰è¿­ä»£è½®æ¬¡ï¼ˆç›´åˆ°å½“å‰è½®æ¬¡ï¼‰
                for iteration in range(1, current_iter + 1):
                    # ä¿å­˜ prompt åˆ° inputs
                    prompt_file = sample_inputs_dir / f"iteration_{iteration}.txt"
                    
                    if sample_idx in state["prompts"] and iteration in state["prompts"][sample_idx]:
                        prompt = state["prompts"][sample_idx][iteration]
                        if not safe_write_file(prompt_file, prompt):
                            logger.error(f"Task {task_id}: å†™å…¥Promptå¤±è´¥ {prompt_file}")
                    else:
                        # å¦‚æœç¼ºå¤±ï¼Œå†™å…¥å ä½ç¬¦ï¼ˆä¾‹å¦‚æ ·æœ¬å¤±è´¥æˆ–è·³è¿‡ï¼‰
                        if not prompt_file.exists():
                            safe_write_file(prompt_file, f"No prompt data for iteration {iteration}")

                    # ä¿å­˜ response åˆ° outputs
                    response_file = sample_outputs_dir / f"iteration_{iteration}.txt"
                    
                    if sample_idx in state["responses"] and iteration in state["responses"][sample_idx]:
                        response_data = state["responses"][sample_idx][iteration]
                        output_content = self._build_response_content(response_data)
                        if not safe_write_file(response_file, output_content):
                             logger.error(f"Task {task_id}: å†™å…¥å“åº”å¤±è´¥ {response_file}")
                    else:
                        # å¦‚æœç¼ºå¤±ï¼Œå†™å…¥å ä½ç¬¦
                        if not response_file.exists():
                            safe_write_file(response_file, f"No response data for iteration {iteration}")

            logger.info(
                f"Task {task_id}: ç¬¬{current_iter}è½® - å·²ä¿å­˜ {len(state['prompts'])} ä¸ªæ ·æœ¬çš„ prompts å’Œ responses"
            )

        except Exception as e:
            logger.error(f"Task {task_id}: ä¿å­˜ prompts å’Œ responses å¤±è´¥: {e}", exc_info=True)

    def _build_response_content(self, response_data: Dict[str, Any]) -> str:
        """
        æ„å»ºå“åº”æ–‡ä»¶å†…å®¹ï¼ˆåªè¿”å› LLM åŸå§‹å“åº”ï¼‰

        Args:
            response_data: å“åº”æ•°æ®

        Returns:
            LLM å“åº”å­—ç¬¦ä¸²
        """
        # ç›´æ¥ä» response_data ä¸­æå– llm_response
        llm_response = response_data.get('llm_response', '')

        # è¿”å›çº¯å‡€çš„ LLM å“åº”å­—ç¬¦ä¸²
        return llm_response if llm_response else "No response available"

    def _should_handle_failure(self, state: IterationState) -> str:
        """
        åˆ¤æ–­æ˜¯å¦éœ€è¦å¤„ç†å¤±è´¥

        Returns:
            "handle_failure" æˆ– "continue"
        """
        if state["failed_samples"]:
            return "handle_failure"
        return "continue"

    def _should_continue_iteration(self, state: IterationState) -> str:
        """
        åˆ¤æ–­æ˜¯å¦ç»§ç»­è¿­ä»£ï¼ˆä¸ä¿®æ”¹çŠ¶æ€ï¼Œåªåšåˆ¤æ–­ï¼‰

        Returns:
            "continue" æˆ– "finish"
        """
        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°
        # æ³¨æ„ï¼šcurrent_iteration åœ¨æ¯è½®ç»“æŸåä¼š +1ï¼Œæ‰€ä»¥è¿™é‡Œç”¨ > è€Œä¸æ˜¯ >=
        # ä¾‹å¦‚ï¼šmax_iterations=3 æ—¶ï¼Œåº”è¯¥æ‰§è¡Œç¬¬1ã€2ã€3è½®ï¼Œç¬¬3è½®ç»“æŸå current_iteration=4ï¼Œæ­¤æ—¶åœæ­¢
        if state["current_iteration"] > state["max_iterations"]:
            logger.info(
                f"Task {state['task_id']}: è¾¾åˆ°æœ€å¤§è¿­ä»£æ¬¡æ•°{state['max_iterations']}ï¼Œåœæ­¢è¿­ä»£"
            )
            return "finish"

        # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰æ ·æœ¬éƒ½å·²æ”¶æ•›æˆ–å¤±è´¥
        total_samples = len(state["test_data"])
        completed_samples = len(state["converged_samples"]) + len(state["failed_samples"])

        if completed_samples >= total_samples:
            logger.info(
                f"Task {state['task_id']}: æ‰€æœ‰æ ·æœ¬å·²å®Œæˆï¼ˆæ”¶æ•›æˆ–å¤±è´¥ï¼‰ï¼Œåœæ­¢è¿­ä»£"
            )
            return "finish"

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨æå‰åœæ­¢
        if state["early_stop"]:
            # å¦‚æœè¶…è¿‡80%çš„æ ·æœ¬å·²æ”¶æ•›ï¼Œå¯ä»¥æå‰åœæ­¢
            convergence_rate = len(state["converged_samples"]) / total_samples
            if convergence_rate >= 0.8:
                logger.info(
                    f"Task {state['task_id']}: æ”¶æ•›ç‡{convergence_rate:.2%}>=80%ï¼Œæå‰åœæ­¢"
                )
                return "finish"

        # ç»§ç»­ä¸‹ä¸€è½®è¿­ä»£
        logger.info(
            f"Task {state['task_id']}: å‡†å¤‡è¿›å…¥ç¬¬{state['current_iteration'] + 1}è½®è¿­ä»£"
        )
        return "continue"

    def _predict_single_sample(
        self,
        state: IterationState,
        sample_idx: int,
        test_sample: Dict[str, Any],
        current_iteration: int
    ) -> Dict[str, Any]:
        """
        é¢„æµ‹å•ä¸ªæ ·æœ¬

        Args:
            state: è¿­ä»£çŠ¶æ€
            sample_idx: æ ·æœ¬ç´¢å¼•
            test_sample: æµ‹è¯•æ ·æœ¬æ•°æ®
            current_iteration: å½“å‰è¿­ä»£è½®æ•°

        Returns:
            åŒ…å«é¢„æµ‹ç»“æœã€Promptã€å“åº”ç­‰ä¿¡æ¯çš„å­—å…¸
        """
        config = state["config"]
        composition = test_sample.get("composition", "")

        # æå–å·¥è‰ºåˆ—
        processing_dict = {}
        if config.get("processing_column"):
            for proc_col in config["processing_column"]:
                if proc_col in test_sample:
                    processing_dict[proc_col] = test_sample[proc_col]

        # æå–ç‰¹å¾åˆ—
        feature_dict = {}
        if config.get("feature_columns"):
            for feat_col in config["feature_columns"]:
                if feat_col in test_sample:
                    feature_dict[feat_col] = test_sample[feat_col]

        # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
        query_text = SampleTextBuilder.build_sample_text(
            composition=composition,
            processing_columns=processing_dict if processing_dict else None,
            feature_columns=feature_dict if feature_dict else None
        )

        # æ£€ç´¢ç›¸ä¼¼æ ·æœ¬
        similar_indices = self.rag_engine.retrieve_similar_samples(
            query_text=query_text,
            train_texts=[s.get("sample_text", "") for s in state["train_data"]],
            train_embeddings=state["train_embeddings"]
        )

        similar_samples = [state["train_data"][i] for i in similar_indices]

        # è·å–åˆ—åæ˜ å°„é…ç½®
        column_name_mapping = None
        if config.get("prompt_template") and "column_name_mapping" in config["prompt_template"]:
            column_name_mapping = config["prompt_template"]["column_name_mapping"]
        else:
            # ä½¿ç”¨é»˜è®¤åˆ—åæ˜ å°„
            from services.prompt_template_manager import PromptTemplateManager
            column_name_mapping = PromptTemplateManager.get_default_column_mapping()

        # æ„å»ºPromptï¼ˆä¼ å…¥åˆ—åæ˜ å°„ï¼‰
        prompt_builder = PromptBuilder(column_name_mapping=column_name_mapping)

        # æ ¼å¼åŒ–è¿­ä»£å†å²ï¼ˆå¦‚æœæ˜¯ç¬¬2è½®åŠä»¥åï¼‰
        iteration_history_str = None
        if current_iteration > 1 and sample_idx in state["iteration_history"]:
            iteration_history_str = prompt_builder.format_multi_target_iteration_history(
                sample_idx,
                state["target_properties"],
                state["iteration_history"][sample_idx]
            )

        # è½¬æ¢ä¸ºPromptBuilderéœ€è¦çš„æ ¼å¼
        retrieved_samples = []
        for sample in similar_samples:
            sample_text = sample.get("sample_text", "")
            retrieved_samples.append((sample_text, 1.0, sample))

        # æ„å»ºPrompt
        prompt = prompt_builder.build_prompt(
            retrieved_samples=retrieved_samples,
            test_sample=query_text,
            target_properties=state["target_properties"],
            iteration=current_iteration,
            iteration_history=iteration_history_str
        )

        # è°ƒç”¨LLMï¼ˆè¿”å›è¯¦ç»†ä¿¡æ¯ä»¥ä¿å­˜å“åº”ï¼‰
        result = self.rag_engine.generate_multi_target_prediction(
            query_composition=composition,
            query_processing=processing_dict if processing_dict else "",
            similar_samples=similar_samples,
            target_columns=state["target_properties"],
            model_provider=state["llm_provider"],
            model_name=state["llm_model"],
            temperature=state["temperature"],
            prompt_template=state["config"].get("prompt_template"),
            return_details=True  # è¿”å›è¯¦ç»†ä¿¡æ¯
        )

        # ä» result ä¸­æå–é¢„æµ‹å€¼ï¼ˆresult æ˜¯è¯¦ç»†ä¿¡æ¯å­—å…¸ï¼‰
        predictions = result.get('predictions', {})

        # æ³¨æ„ï¼šè¿™é‡Œä¸å†æŠ›å‡ºå¼‚å¸¸ï¼Œè€Œæ˜¯è¿”å›ç»“æœï¼Œç”±è°ƒç”¨æ–¹(_run_parallel_predictions)æ£€æŸ¥æ˜¯å¦å…¨ä¸º0
        # è¿™æ ·å¯ä»¥ç¡®ä¿å³ä½¿é¢„æµ‹å¤±è´¥ï¼ŒPromptå’ŒResponseä¹Ÿèƒ½è¢«ä¿å­˜

        # å¯¹ç›¸ä¼¼æ ·æœ¬è¿›è¡Œå¤„ç†ï¼š
        # 1. åº”ç”¨åˆ—åæ˜ å°„åˆ° sample_text
        # 2. åªä¿ç•™ sample_text å’Œç›®æ ‡å±æ€§ï¼Œç§»é™¤å…¶ä»–å­—æ®µï¼ˆå¦‚ Processing_Descriptionï¼‰
        mapped_similar_samples = []
        for sample in similar_samples:
            clean_sample = {}
            
            # å¤„ç† sample_text
            original_text = sample.get("sample_text", "")
            if original_text:
                clean_sample["sample_text"] = prompt_builder._apply_column_name_mapping(original_text)
            
            # ä¿ç•™ç›®æ ‡å±æ€§
            for target in state["target_properties"]:
                if target in sample:
                    clean_sample[target] = sample[target]
            
            mapped_similar_samples.append(clean_sample)

        # æ„å»ºå“åº”æ•°æ®
        response_data = {
            "predictions": predictions,
            "confidence": result.get('confidence'),
            "llm_response": result.get('llm_response', ''),
            "prompt": prompt,
            "similar_samples": mapped_similar_samples,
            "similar_samples_count": len(similar_samples)
        }

        return {
            "predictions": predictions,
            "prompt": prompt,
            "response_data": response_data,
            "mapped_similar_samples": mapped_similar_samples
        }

    def run_iterative_prediction(
        self,
        task_id: str,
        config: PredictionConfig,
        train_data: List[Dict[str, Any]],
        test_data: List[Dict[str, Any]],
        train_embeddings: Any
    ) -> Dict[str, Any]:
        """
        è¿è¡Œè¿­ä»£é¢„æµ‹

        Args:
            task_id: ä»»åŠ¡ID
            config: é¢„æµ‹é…ç½®
            train_data: è®­ç»ƒæ•°æ®
            test_data: æµ‹è¯•æ•°æ®
            train_embeddings: è®­ç»ƒæ•°æ®åµŒå…¥

        Returns:
            è¿­ä»£é¢„æµ‹ç»“æœ
        """
        logger.info(f"Task {task_id}: å¼€å§‹è¿è¡Œè¿­ä»£é¢„æµ‹å·¥ä½œæµ")

        # åˆå§‹åŒ–çŠ¶æ€
        initial_state: IterationState = {
            "task_id": task_id,
            "config": config.model_dump(),
            "train_data": train_data,
            "test_data": test_data,
            "train_embeddings": train_embeddings,
            "current_iteration": 1,
            "max_iterations": config.max_iterations,
            "convergence_threshold": config.convergence_threshold,
            "early_stop": config.early_stop,
            "iteration_results": {},
            "iteration_history": {},
            "converged_samples": set(),
            "failed_samples": {},
            "llm_provider": config.model_provider or "gemini",
            "llm_model": config.model_name or "gemini-2.5-flash",
            "temperature": config.temperature or 1.0,
            "start_time": datetime.now(),
            "iteration_start_times": {},
            "max_workers": config.max_workers,
            "target_properties": config.target_columns,
            "sample_size": config.sample_size,
            "prompts": {},
            "responses": {}
        }

        try:
            # è®¾ç½®é€’å½’é™åˆ¶ï¼ˆæœ€å¤§è¿­ä»£æ¬¡æ•° * 10ï¼Œå› ä¸ºæ¯è½®è¿­ä»£ä¼šç»è¿‡å¤šä¸ªèŠ‚ç‚¹ï¼‰
            recursion_limit = max(config.max_iterations * 10, 100)
            logger.info(f"Task {task_id}: å·¥ä½œæµé€’å½’é™åˆ¶è®¾ç½®ä¸º {recursion_limit}")

            final_state = self.workflow.invoke(
                initial_state,
                config={"recursion_limit": recursion_limit}
            )

            logger.info(f"Task {task_id}: è¿­ä»£é¢„æµ‹å·¥ä½œæµå®Œæˆ")

            return {
                "success": True,
                "total_iterations": final_state["current_iteration"],
                "converged_samples": len(final_state["converged_samples"]),
                "failed_samples": len(final_state["failed_samples"]),
                "iteration_history": final_state["iteration_history"]
            }

        except Exception as e:
            logger.error(f"Task {task_id}: è¿­ä»£é¢„æµ‹å·¥ä½œæµå¤±è´¥: {e}", exc_info=True)

            # æ›´æ–°ä»»åŠ¡çŠ¶æ€ä¸ºå¤±è´¥
            self.task_manager.update_task(
                task_id,
                {
                    "status": TaskStatus.FAILED,
                    "error": str(e)
                }
            )

            return {
                "success": False,
                "error": str(e)
            }

